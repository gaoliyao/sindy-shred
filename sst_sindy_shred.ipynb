{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SINDy-SHRED applied to SST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This iPython notebook gives an introductory walkthrough to using SHRED models.  The dataset we consider is weekly mean sea-surface temperature as given by the NOAA Optimum Interpolation SST V2 dataset (https://psl.noaa.gov/data/gridded/data.noaa.oisst.v2.html).\n",
    "\n",
    "SHRED (SHallow REcurrent Decoder) models are a network architecture that merges a recurrent layer (LSTM) with a shallow decoder network (SDN) to reconstruct high-dimensional spatio-temporal fields from a trajectory of sensor measurements of the field. More formally, the SHRED architecture can be written as \n",
    "$$ \\mathcal {H} \\left( \\{ y_i \\} _{i=t-k}^t \\right) = \\mathcal {F} \\left( \\mathcal {G} \\left( \\{ y_i \\} _{i=t-k}^t \\right) ; W_{RN}) ; W_{SD} \\right)$$\n",
    "where $\\mathcal F$ is a feed forward network parameterized by weights $W_{SD}$, $\\mathcal G$ is a LSTM network parameterized by weights $W_{RN}$, and $\\{ y_i \\} _{i=t-k}^t$ is a trajectory of sensor measurements of a high-dimensional spatio-temporal field $\\{ x_i \\} _{i=t-k}^t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SINDy-SHRED further extends the SHRED architecture by integrating **Sparse Identification of Nonlinear Dynamics (SINDy)** into the recurrent decoder framework. The key idea is to enforce a **parsimonious latent space representation**, where the dynamics of the latent variables are governed by a **sparse set of basis functions**. \n",
    "\n",
    "More formally, SINDy-SHRED introduces an additional inductive bias:\n",
    "\n",
    "$$\\dot{z} = \\Theta(z) \\xi$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $z = \\mathcal{G} \\left( \\{ y_i \\} _{i=t-k}^t \\right)$ represents the **latent space variables** extracted by the LSTM encoder.\n",
    "- $\\Theta(z)$ is a **library of candidate nonlinear functions**.\n",
    "- $\\xi$ is a **sparse coefficient matrix** that determines the governing equations.\n",
    "\n",
    "This combination enables **interpretable spatio-temporal modeling** by ensuring that the learned representations adhere to a **governing law**, making the method suitable for **data-driven discovery of dynamical systems** from high-dimensional, noisy observations.\n",
    "\n",
    "In this notebook, we will walk through the application of **SINDy-SHRED** on the **weekly mean sea-surface temperature (SST) dataset**, demonstrating how the model can **learn a reduced-order latent representation** and **recover the governing equations** that describe the SST evolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first randomly select 3 sensor locations and set the trajectory length (lags) to 52, corresponding to one year of measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from processdata import load_data\n",
    "from processdata import TimeSeriesDataset\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import sindy\n",
    "\n",
    "num_sensors = 250\n",
    "lags = 52\n",
    "load_X = load_data('SST')\n",
    "n = load_X.shape[0]\n",
    "m = load_X.shape[1]\n",
    "sensor_locations = np.random.choice(m, size=num_sensors, replace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Specify your device here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set as CPU (default)\n",
    "# device = 'cpu'\n",
    "\n",
    "# Automatically select GPU if available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Change to GPU with device ID\n",
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now select indices to divide the data into training, validation, and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = np.arange(0,1000)\n",
    "mask = np.ones(n - lags)\n",
    "mask[train_indices] = 0\n",
    "valid_test_indices = np.arange(0, n - lags)[np.where(mask!=0)[0]]\n",
    "valid_indices = valid_test_indices[:30]\n",
    "test_indices = valid_test_indices[30:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn's MinMaxScaler is used to preprocess the data for training and we generate input/output pairs for the training, validation, and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = MinMaxScaler()\n",
    "sc = sc.fit(load_X[train_indices])\n",
    "transformed_X = sc.transform(load_X)\n",
    "\n",
    "### Generate input sequences to a SHRED model\n",
    "all_data_in = np.zeros((n - lags, lags, num_sensors))\n",
    "for i in range(len(all_data_in)):\n",
    "    all_data_in[i] = transformed_X[i:i+lags, sensor_locations]\n",
    "\n",
    "### Generate training validation and test datasets both for reconstruction of states and forecasting sensors\n",
    "train_data_in = torch.tensor(all_data_in[train_indices], dtype=torch.float32).to(device)\n",
    "valid_data_in = torch.tensor(all_data_in[valid_indices], dtype=torch.float32).to(device)\n",
    "test_data_in = torch.tensor(all_data_in[test_indices], dtype=torch.float32).to(device)\n",
    "\n",
    "### -1 to have output be at the same time as final sensor measurements\n",
    "train_data_out = torch.tensor(transformed_X[train_indices + lags - 1], dtype=torch.float32).to(device)\n",
    "valid_data_out = torch.tensor(transformed_X[valid_indices + lags - 1], dtype=torch.float32).to(device)\n",
    "test_data_out = torch.tensor(transformed_X[test_indices + lags - 1], dtype=torch.float32).to(device)\n",
    "\n",
    "train_dataset = TimeSeriesDataset(train_data_in, train_data_out)\n",
    "valid_dataset = TimeSeriesDataset(valid_data_in, valid_data_out)\n",
    "test_dataset = TimeSeriesDataset(test_data_in, test_data_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_index_X = transformed_X[:,sensor_locations]\n",
    "fig, ax = plt.subplots(min(num_sensors,10))\n",
    "\n",
    "for i in range(min(num_sensors,10)):\n",
    "    ax[i].plot(sub_index_X[:,i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set up SINDy library coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 3\n",
    "poly_order = 1\n",
    "include_sine = False\n",
    "library_dim = sindy.library_size(latent_dim, poly_order, include_sine, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the model using the training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sindy_shred\n",
    "import importlib\n",
    "importlib.reload(sindy_shred)\n",
    "importlib.reload(sindy)\n",
    "shred = sindy_shred.SINDy_SHRED(num_sensors, m, hidden_size=latent_dim, hidden_layers=2, l1=350, l2=400, dropout=0.1, \n",
    "                                 library_dim=library_dim, poly_order=poly_order, include_sine=include_sine, dt=1/52.0, device=device).to(device)\n",
    "validation_errors = sindy_shred.fit(shred, train_dataset, valid_dataset, batch_size=128, num_epochs=600, lr=1e-3, verbose=True, threshold=0.05, patience=5, sindy_regularization=10.0, optimizer=\"AdamW\", thres_epoch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Rate of sparsity\n",
    "torch.mean(shred.e_sindy.coefficient_mask*1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then generate reconstructions from the test set and print mean square error compared to the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_recons = sc.inverse_transform(shred(test_dataset.X).detach().cpu().numpy())\n",
    "test_ground_truth = sc.inverse_transform(test_dataset.Y.detach().cpu().numpy())\n",
    "print(np.linalg.norm(test_recons - test_ground_truth) / np.linalg.norm(test_ground_truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysindy as ps\n",
    "from pysindy.differentiation import FiniteDifference\n",
    "gru_outs, sindy_outs = shred.gru_outputs(test_dataset.X, sindy=True)\n",
    "differentiation_method = FiniteDifference()\n",
    "\n",
    "fig, ax = plt.subplots(latent_dim)\n",
    "for i in range(latent_dim):\n",
    "    ax[i].plot(gru_outs[1:,0,i].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model discovery with SINDy\n",
    "\n",
    "The following code block extracts latent states using a GRU model. Each of the first three dimensions is normalized and re-scaled to [-1,1]. A SINDy model is then set up with a polynomial library. This SINDy model is fitted to the data and later used to simulate the dynamics for comparison with the original GRU latent trajectories.\n",
    "\n",
    "The hyperparameters for the SINDy model are set as follows:\n",
    "- **Differentiation Method:** Finite difference is used to compute numerical derivatives. When the latent states trajectories are noisy, one should consider to use ps.differentiation.SmoothedFiniteDifference().\n",
    "- **Optimizer Sequantially thresholded least-squares: STLSQ:**  \n",
    "  - *Threshold:* 0.8, which controls the sparsity level by eliminating coefficients below this value.\n",
    "  - *Alpha:* 0.05, L2 regularization.\n",
    "- **Optimizer Mixed-interger optimization: MIOSR:**  \n",
    "  - *group_sparsity:* how many termseach equations should include. \n",
    "- **Feature Library:** Here we only include polynomial features up to degree 1, ensuring only linear terms are considered in the model. Practically this could set up to degree 3 which will include more nonlinear terms. \n",
    "\n",
    "For model selection, a simple baseline standard is to visually examine how the long-term extrapolation behaves. We can pick the model that fits the data well. It is also possible to include metrics (like the MSE) to quantitatively measure these quantities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_outs, sindy_outs = shred.gru_outputs(train_dataset.X, sindy=True)\n",
    "gru_outs = gru_outs[:,0,:]\n",
    "\n",
    "poly_order = 1\n",
    "threshold = 0.05\n",
    "\n",
    "###############Normalization###############\n",
    "\n",
    "for ii in range(latent_dim):\n",
    "    gru_outs[:,ii] = (gru_outs[:,ii] - torch.min(gru_outs[:,ii])) / (torch.max(gru_outs[:,ii])-torch.min(gru_outs[:,ii]))\n",
    "gru_outs = 2 * gru_outs - 1\n",
    "\n",
    "###############SINDy discovery###############\n",
    "\n",
    "x = gru_outs.detach().cpu().numpy()\n",
    "\n",
    "differentiation_method = ps.differentiation.FiniteDifference()\n",
    "# differentiation_method = ps.differentiation.SmoothedFiniteDifference()\n",
    "\n",
    "model = ps.SINDy(\n",
    "    optimizer=ps.STLSQ(threshold=0.0, alpha=0.05),\n",
    "    differentiation_method=differentiation_method,\n",
    "    feature_library=ps.PolynomialLibrary(degree=poly_order)\n",
    ")\n",
    "\n",
    "# model = ps.SINDy(\n",
    "#     optimizer=ps.MIOSR(group_sparsity=(2,2,2), alpha=5000),\n",
    "#     differentiation_method=differentiation_method,\n",
    "#     feature_library=ps.PolynomialLibrary(degree=poly_order),\n",
    "# )\n",
    "\n",
    "model.fit(x, t=1/52.0)\n",
    "model.print()\n",
    "\n",
    "\n",
    "###############Plot the discovered SINDy model###############\n",
    "\n",
    "t_train = np.arange(0, 20, 1/52.0)\n",
    "init_cond = np.zeros(latent_dim)\n",
    "init_cond[:latent_dim] = gru_outs[0,:].detach().cpu().numpy()\n",
    "x_sim = model.simulate(init_cond, t_train)\n",
    "\n",
    "fig, ax = plt.subplots(latent_dim)\n",
    "for i in range(latent_dim):\n",
    "    ax[i].plot(gru_outs[:,i].detach().cpu().numpy())\n",
    "    ax[i].plot(x_sim[:,i], \"k--\", label=\"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reload data from the entire training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_outs_train, _ = shred.gru_outputs(train_dataset.X, sindy=True)\n",
    "gru_outs_train = gru_outs_train[:,0,:]\n",
    "gru_outs_val, _ = shred.gru_outputs(valid_dataset.X, sindy=True)\n",
    "gru_outs_val = gru_outs_val[:,0,:]\n",
    "gru_outs_test, _ = shred.gru_outputs(test_dataset.X, sindy=True)\n",
    "gru_outs_test = gru_outs_test[:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_outs_all = np.zeros((1348-3, latent_dim))\n",
    "gru_outs_all[:1000-1,:] = gru_outs_train.detach().cpu().numpy()\n",
    "gru_outs_all[1000-1:1030-2,:] = gru_outs_val.detach().cpu().numpy()\n",
    "gru_outs_all[1030-2:,:] = gru_outs_test.detach().cpu().numpy()\n",
    "\n",
    "gru_outs_numpy = gru_outs_train.detach().cpu().numpy()\n",
    "\n",
    "gru_outs_all[:,0] = (gru_outs_all[:,0] - np.min(gru_outs_numpy[:,0])) / (np.max(gru_outs_numpy[:,0])-np.min(gru_outs_numpy[:,0]))\n",
    "gru_outs_all[:,1] = (gru_outs_all[:,1] - np.min(gru_outs_numpy[:,1])) / (np.max(gru_outs_numpy[:,1])-np.min(gru_outs_numpy[:,1]))\n",
    "gru_outs_all[:,2] = (gru_outs_all[:,2] - np.min(gru_outs_numpy[:,2])) / (np.max(gru_outs_numpy[:,2])-np.min(gru_outs_numpy[:,2]))\n",
    "\n",
    "gru_outs_all = 2 * gru_outs_all - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "\n",
    "###############Normalization###############\n",
    "gru_outs_test_np = gru_outs_test.detach().cpu().numpy()\n",
    "\n",
    "for ii in range(latent_dim):\n",
    "    gru_outs_test_np[:, ii] = (gru_outs_test_np[:, ii] - np.min(gru_outs_numpy[:, ii])) / (np.max(gru_outs_numpy[:, ii]) - np.min(gru_outs_numpy[:, ii]))\n",
    "gru_outs_test_np = 2 * gru_outs_test_np - 1  # Transform to [-1, 1]\n",
    "\n",
    "################Forward simulation with the model################\n",
    "t_train = np.arange(0, 317*1/52.0, 1/52.0)\n",
    "init_cond = np.zeros(latent_dim)\n",
    "init_cond[:latent_dim] = gru_outs_test_np[0, :]\n",
    "x_sim_test = model.simulate(init_cond, t_train)\n",
    "\n",
    "# Plotting for each latent dimension: True vs SINDy\n",
    "fig, ax = plt.subplots(latent_dim, figsize=(10, latent_dim * 3))\n",
    "\n",
    "for i in range(latent_dim):\n",
    "    ax[i].plot(gru_outs_test_np[:, i], label='True Signal', linewidth=2)\n",
    "    ax[i].plot(x_sim_test[:, i], \"k--\", label='SINDy Approximation', linewidth=2)\n",
    "    ax[i].legend()\n",
    "    ax[i].set_ylabel(f'z_{i+1}')\n",
    "    ax[i].set_xlabel('Time')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "###############Predict back in the pixel space###############\n",
    "\n",
    "# Step 1: Revert the normalization to original scale\n",
    "gru_outs_test_np = (gru_outs_test_np + 1) / 2  # Revert from [-1, 1] to [0, 1]\n",
    "\n",
    "for i in range(3):  # Assuming 3 latent dimensions for this example\n",
    "    gru_outs_test_np[:, i] = gru_outs_test_np[:, i] * (np.max(gru_outs_numpy[:, i]) - np.min(gru_outs_numpy[:, i])) + np.min(gru_outs_numpy[:, i])\n",
    "\n",
    "# Step 2: Decoder reconstruction using the decoder model\n",
    "latent_pred = torch.FloatTensor(gru_outs_test_np).to(device)\n",
    "\n",
    "# Pass through the decoder\n",
    "output = shred.linear1(latent_pred)\n",
    "output = shred.dropout(output)\n",
    "output = torch.nn.functional.relu(output)\n",
    "output = shred.linear2(output)\n",
    "output = shred.dropout(output)\n",
    "output = torch.nn.functional.relu(output)\n",
    "output = shred.linear3(output)\n",
    "\n",
    "output_np = output.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############Plotting settings###############\n",
    "load_X = loadmat('Data/SST_data.mat')['Z'].T\n",
    "mean_X = np.mean(load_X, axis=0)\n",
    "sst_locs = np.where(mean_X != 0)[0]\n",
    "reconstructed_data = np.zeros_like(load_X[0, :])\n",
    "reconstructed_data[sst_locs] = output_np[0, :]  # Assuming first timestep\n",
    "reshaped_reconstructed = reconstructed_data.reshape(180, 360)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "\n",
    "def reconstruct_and_plot_sindy(x_sim_test, gru_outs_numpy, decoder_model, sst_data_path, timesteps):\n",
    "    \"\"\"\n",
    "    Reconstructs and visualizes data using SINDy-simulated latent space for multiple timesteps.\n",
    "    \n",
    "    Args:\n",
    "        x_sim_test (np.array): SINDy-simulated latent space.\n",
    "        gru_outs_numpy (np.array): The original latent space for Min-Max scaling reversal.\n",
    "        decoder_model (nn.Module): The decoder model for reconstruction.\n",
    "        sst_data_path (str): Path to the SST data file.\n",
    "        timesteps (list of int): List of timesteps to plot.\n",
    "\n",
    "    Returns:\n",
    "        None. It generates and shows the plots.\n",
    "    \"\"\"\n",
    "    # Step 1: Reverse Min-Max scaling for SINDy-simulated data (x_sim_test)\n",
    "    x_sim_test = np.array(x_sim_test)  # Ensure it's a numpy array if needed\n",
    "\n",
    "    # Revert the scaling from [-1, 1] back to [0, 1]\n",
    "    x_sim_test = (x_sim_test + 1) / 2  \n",
    "\n",
    "    # Perform the Min-Max reverse transformation using the original min/max values\n",
    "    for i in range(3):  # Assuming 3 latent dimensions, need to be updated\n",
    "        x_sim_test[:, i] = x_sim_test[:, i] * (np.max(gru_outs_numpy[:, i]) - np.min(gru_outs_numpy[:, i])) + np.min(gru_outs_numpy[:, i])\n",
    "\n",
    "    # Perform the decoder reconstruction using the transformed SINDy-simulated data\n",
    "    latent_pred_sindy = torch.FloatTensor(x_sim_test).to(device)  # Convert to torch tensor for reconstruction\n",
    "\n",
    "    # Pass the SINDy-simulated latent space data through the decoder\n",
    "    output_sindy = decoder_model.linear1(latent_pred_sindy)\n",
    "    output_sindy = decoder_model.dropout(output_sindy)\n",
    "    output_sindy = torch.nn.functional.relu(output_sindy)\n",
    "    output_sindy = decoder_model.linear2(output_sindy)\n",
    "    output_sindy = decoder_model.dropout(output_sindy)\n",
    "    output_sindy = torch.nn.functional.relu(output_sindy)\n",
    "    output_sindy = decoder_model.linear3(output_sindy)\n",
    "\n",
    "    # Detach and convert the reconstructed data back to numpy for visualization\n",
    "    output_sindy_np = output_sindy.detach().cpu().numpy()\n",
    "    load_X = loadmat(sst_data_path)['Z'].T\n",
    "    mean_X = np.mean(load_X, axis=0)\n",
    "    sst_locs = np.where(mean_X != 0)[0]\n",
    "\n",
    "    # Prepare the SST data reconstruction and plot for each timestep\n",
    "    for t in timesteps:\n",
    "        # Initialize an empty array with the same shape as the original SST data\n",
    "        reconstructed_data_sindy = np.zeros_like(load_X[0, :])\n",
    "\n",
    "        # Copy the SINDy-reconstructed values back to the SST locations for the current timestep\n",
    "        reconstructed_data_sindy[sst_locs] = output_sindy_np[t, :]  # Use the selected timestep\n",
    "        print(np.sum(np.square(output_sindy_np[t,:]-output_sindy_np[t+1, :])))\n",
    "\n",
    "        # Reshape the SST data back to its original 2D form (180, 360)\n",
    "        reshaped_reconstructed_sindy = reconstructed_data_sindy.reshape(180, 360)\n",
    "\n",
    "        # Step 5: Visualization of the SST data reconstructed using the SINDy-simulated latent space\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.imshow(reshaped_reconstructed_sindy, aspect='auto', interpolation='nearest')\n",
    "        plt.colorbar()  # Add a color bar to show the value range\n",
    "\n",
    "        # Remove axes, ticks, and background for a clean image\n",
    "        plt.gca().set_axis_off()\n",
    "\n",
    "        # Show the plot with transparent background\n",
    "        plt.tight_layout(pad=0)\n",
    "        plt.title(f'Reconstructed SST Data at Timestep {t}')\n",
    "        plt.show()\n",
    "    return output_sindy_np\n",
    "\n",
    "# Example usage\n",
    "# Assuming decoder_model is your decoder, and timesteps are [0, 10, 50] or any set of timesteps you'd like to plot\n",
    "timesteps = [16]  # Define timesteps to plot\n",
    "output_sindy_np = reconstruct_and_plot_sindy(x_sim_test, gru_outs_numpy, shred, 'Data/SST_data.mat', timesteps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the true data to predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_zoomed_comparison(real_data, sindy_data, sst_locs, timesteps, lat_range, lon_range, diff_scale=10):\n",
    "    \"\"\"\n",
    "    Plot the real SST data, SINDy-SHRED predictions, and their differences in a specific area (zoomed).\n",
    "    The difference is amplified by a scaling factor to make it clearer.\n",
    "\n",
    "    Args:\n",
    "        real_data (np.array): The ground truth SST data.\n",
    "        sindy_data (np.array): The SINDy-SHRED reconstructed data.\n",
    "        sst_locs (np.array): Indices of non-zero SST locations.\n",
    "        timesteps (list of int): List of timesteps to visualize.\n",
    "        lat_range (tuple): Latitude range for zoom (e.g., (start_lat, end_lat)).\n",
    "        lon_range (tuple): Longitude range for zoom (e.g., (start_lon, end_lon)).\n",
    "        diff_scale (float): Scaling factor for amplifying the difference plot.\n",
    "\n",
    "    Returns:\n",
    "        None. Displays the comparison plots.\n",
    "    \"\"\"\n",
    "    num_plots = len(timesteps)\n",
    "    \n",
    "    fig, axes = plt.subplots(3, num_plots, figsize=(15, 10))\n",
    "    \n",
    "    # real_data = sc.inverse_transform(real_data)\n",
    "    # sindy_data = sc.inverse_transform(sindy_data)\n",
    "    # vmin = min(real_data.min(), sindy_data.min())\n",
    "    # vmax = max(real_data.max(), sindy_data.max())\n",
    "    vmin = np.percentile(np.concatenate([real_data, sindy_data]), 1)\n",
    "    vmax = np.percentile(np.concatenate([real_data, sindy_data]), 99)\n",
    "    for i, t in enumerate(timesteps):\n",
    "        # Plot Real Data\n",
    "        real_sst = np.zeros((180 * 360))\n",
    "        real_sst[sst_locs] = 1*(real_data[t, :])\n",
    "        reshaped_real_sst = real_sst.reshape(180, 360)\n",
    "        \n",
    "        # Select the region of interest (lat_range, lon_range)\n",
    "        real_sst_zoomed = reshaped_real_sst[lat_range[0]:lat_range[1], lon_range[0]:lon_range[1]]\n",
    "        axes[0, i].imshow(real_sst_zoomed, aspect='auto', cmap='twilight', interpolation='none', vmin=vmin, vmax=vmax)\n",
    "        axes[0, i].set_title(f'Real Data (Timestep {t})')\n",
    "        axes[0, i].set_axis_off()\n",
    "\n",
    "        # Plot SINDy-SHRED Prediction\n",
    "        sindy_sst = np.zeros((180 * 360))\n",
    "        sindy_sst[sst_locs] = 1*(sindy_data[t, :])\n",
    "        reshaped_sindy_sst = sindy_sst.reshape(180, 360)\n",
    "        \n",
    "        # Select the region of interest (lat_range, lon_range)\n",
    "        sindy_sst_zoomed = reshaped_sindy_sst[lat_range[0]:lat_range[1], lon_range[0]:lon_range[1]]\n",
    "        axes[1, i].imshow(sindy_sst_zoomed, aspect='auto', cmap='twilight', interpolation='none', vmin=vmin, vmax=vmax)\n",
    "        axes[1, i].set_title(f'SINDy-SHRED Prediction (Timestep {t})')\n",
    "        axes[1, i].set_axis_off()\n",
    "\n",
    "        # Plot Difference (Real - SINDy) with scaling factor\n",
    "        diff_sst_zoomed = 10*np.square(real_sst_zoomed - sindy_sst_zoomed)\n",
    "        axes[2, i].imshow(diff_sst_zoomed, aspect='auto', cmap='plasma', interpolation='none')  # Blue-White-Red for differences\n",
    "        axes[2, i].set_title(f'Difference (Timestep {t}) [Scaled]')\n",
    "        axes[2, i].set_axis_off()\n",
    "        \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "timesteps = [0, 50, 75, 100, 125]  # Define the timesteps you want to compare\n",
    "lat_range = (0, 180)  # Define the latitude range to zoom in (adjust based on your grid)\n",
    "lon_range = (0, 180)  # Define the longitude range to zoom in (adjust based on your grid)\n",
    "plot_zoomed_comparison(test_dataset.Y.detach().cpu().numpy(), output_sindy_np, sst_locs, timesteps, lat_range, lon_range, diff_scale=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rescaled_latent_reconstructions(decoder_model, sst_data_path, gru_outs_numpy):\n",
    "    \"\"\"\n",
    "    Rescales and plots the decoder reconstruction for specific latent space vectors (1,0,0), (0,1,0), and (0,0,1).\n",
    "    Keeps consistent numerical scales and visualization styles.\n",
    "\n",
    "    Args:\n",
    "        decoder_model (nn.Module): The decoder model for reconstruction.\n",
    "        sst_data_path (str): Path to the SST data file.\n",
    "        gru_outs_numpy (np.array): Original latent space data for Min-Max scaling reversal.\n",
    "\n",
    "    Returns:\n",
    "        None. Displays the plots.\n",
    "    \"\"\"\n",
    "    # Define the specific latent space vectors\n",
    "    latent_vectors = np.eye(3)  # (1,0,0), (0,1,0), (0,0,1)\n",
    "    # Convert latent vectors to a torch tensor\n",
    "    latent_vectors_tensor = torch.FloatTensor(latent_vectors).to(device)\n",
    "    \n",
    "    for i in range(3):  # Assuming 3 latent dimensions\n",
    "        latent_vectors_tensor[:, i] = latent_vectors_tensor[:, i] * (np.max(gru_outs_numpy[:, i]) - np.min(gru_outs_numpy[:, i])) + np.min(gru_outs_numpy[:, i])\n",
    "\n",
    "    # Perform reconstruction using the decoder\n",
    "    output_reconstructed = decoder_model.linear1(latent_vectors_tensor)\n",
    "    output_reconstructed = decoder_model.dropout(output_reconstructed)\n",
    "    output_reconstructed = torch.nn.functional.relu(output_reconstructed)\n",
    "    output_reconstructed = decoder_model.linear2(output_reconstructed)\n",
    "    output_reconstructed = decoder_model.dropout(output_reconstructed)\n",
    "    output_reconstructed = torch.nn.functional.relu(output_reconstructed)\n",
    "    output_reconstructed = decoder_model.linear3(output_reconstructed)\n",
    "\n",
    "    # Convert reconstructed data back to numpy\n",
    "    output_reconstructed_np = output_reconstructed.detach().cpu().numpy()\n",
    "    \n",
    "    # Load the SST data to determine grid locations\n",
    "    load_X = loadmat(sst_data_path)['Z'].T\n",
    "    mean_X = np.mean(load_X, axis=0)\n",
    "    sst_locs = np.where(mean_X != 0)[0]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "    # Plot each reconstructed result\n",
    "    for i, vec in enumerate(latent_vectors):\n",
    "        # Initialize an empty array for the entire SST grid\n",
    "        reconstructed_sst = np.zeros_like(load_X[0, :]) - 0.25\n",
    "\n",
    "        # Fill the SST grid with the reconstructed values\n",
    "        reconstructed_sst[sst_locs] = output_reconstructed_np[i, :]\n",
    "\n",
    "        # Reshape the SST data to 2D (180, 360)\n",
    "        reshaped_sst = reconstructed_sst.reshape(180, 360)\n",
    "        \n",
    "        reshaped_sst = reshaped_sst[lat_range[0]:lat_range[1], lon_range[0]:lon_range[1]]\n",
    "        axes[i].imshow(reshaped_sst, aspect='auto', cmap='twilight', interpolation='none')\n",
    "        axes[i].set_axis_off()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_rescaled_latent_reconstructions(\n",
    "    decoder_model=shred,\n",
    "    sst_data_path='Data/SST_data.mat',\n",
    "    gru_outs_numpy=gru_outs_numpy\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "timesteps = [25, 50, 75]  # Define the timesteps you want to compare\n",
    "lat_range = (0, 180)  # Define the latitude range to zoom in (adjust based on your grid)\n",
    "lon_range = (0, 360)  # Define the longitude range to zoom in (adjust based on your grid)\n",
    "plot_zoomed_comparison(test_dataset.Y.detach().cpu().numpy(), output_sindy_np, sst_locs, timesteps, lat_range, lon_range, diff_scale=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensor-level plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_multiple_sensor_predictions(real_data, sindy_data, sensor_locations, sensor_indices, num_train=52, num_pred=250, rows=4, cols=4):\n",
    "    \"\"\"\n",
    "    Plot the real data and SINDy-SHRED prediction for multiple sensors in a grid layout.\n",
    "    \n",
    "    Args:\n",
    "        real_data (np.array): The real SST data.\n",
    "        sindy_data (np.array): The predicted data from SINDy-SHRED.\n",
    "        sensor_locations (np.array): Sensor locations (indices) used in training.\n",
    "        sensor_indices (list): Indices of the sensors to visualize.\n",
    "        num_train (int): Number of training data points (default is 100).\n",
    "        num_pred (int): Number of prediction data points (default is 100).\n",
    "        rows (int): Number of rows in the grid layout.\n",
    "        cols (int): Number of columns in the grid layout.\n",
    "    \n",
    "    Returns:\n",
    "        None. Displays and saves the plot.\n",
    "    \"\"\"\n",
    "    num_sensors = len(sensor_indices)\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(3*cols, 2*rows), sharex=True, sharey=True)\n",
    "    axes = axes.flatten()  # Flatten the 2D array of axes for easy indexing\n",
    "    \n",
    "    for i, sensor_idx in enumerate(sensor_indices):\n",
    "        if i >= rows * cols:\n",
    "            break  # Stop if we've exceeded the number of subplots\n",
    "        \n",
    "        sensor = sensor_locations[sensor_idx]\n",
    "        \n",
    "        # Real data for the training period\n",
    "        sensor_real_data = real_data[:num_train+num_pred, sensor]\n",
    "        \n",
    "        # SINDy-SHRED prediction for the prediction period\n",
    "        sensor_pred_data = sindy_data[num_train:num_train+num_pred, sensor]\n",
    "        \n",
    "        axes[i].plot(np.arange(num_train+num_pred), sensor_real_data, color=\"blue\", linewidth=4)\n",
    "        axes[i].plot(np.arange(num_train, num_train+num_pred), sensor_pred_data, color=\"red\", linestyle=\"--\", linewidth=4)\n",
    "        \n",
    "        axes[i].set_title(f'Sensor {sensor_idx}', fontsize=15)\n",
    "        axes[i].tick_params(axis='both', which='major', labelsize=15)\n",
    "        \n",
    "        # Add a vertical line to separate training and prediction periods\n",
    "        axes[i].axvline(x=num_train, color='gray', linestyle=':', linewidth=2)\n",
    "        \n",
    "    # Remove any unused subplots\n",
    "    for i in range(num_sensors, rows * cols):\n",
    "        fig.delaxes(axes[i])\n",
    "    \n",
    "    # Add labels and title\n",
    "    fig.text(0.5, 0.0, 'Time (weeks)', ha='center', va='center', fontsize=20)\n",
    "#     fig.text(0.02, 0.5, 'Sensor Value', ha='center', va='center', rotation='vertical', fontsize=10)\n",
    "#     fig.suptitle(\"Real Data (blue) vs SINDy-SHRED Prediction (red dashed)\", fontsize=14)\n",
    "    \n",
    "    # Add a common legend\n",
    "    lines = [plt.Line2D([0], [0], color=\"blue\", lw=4),\n",
    "             plt.Line2D([0], [0], color=\"red\", linestyle=\"--\", lw=4)]\n",
    "    labels = [\"Real Data\", \"SINDy-SHRED\"]\n",
    "    fig.legend(lines, labels, loc=\"upper right\", bbox_to_anchor=(0.99, 0.945), fontsize=8, frameon=False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('sensor_predictions_grid.pdf', format='pdf', bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "sensor_locations_test = np.random.randint(1, 40000, size=18)\n",
    "sensor_indices = list(range(0, 18, 1))  # Choose 32 sensors (0, 5, 10, ..., 155)\n",
    "plot_multiple_sensor_predictions(test_dataset.Y.detach().cpu().numpy(), output_sindy_np, sensor_locations_test, sensor_indices, rows=3, cols=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sindyshred",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
