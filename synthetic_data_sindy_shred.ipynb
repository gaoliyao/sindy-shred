{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-header",
   "metadata": {},
   "source": [
    "# SINDy-SHRED: Synthetic Data Example (Low-Level API)\n",
    "\n",
    "This notebook demonstrates the detailed workflow of SINDy-SHRED on synthetic data.\n",
    "For a simpler high-level interface, see `synthetic_data_sindy_shred_refactor.ipynb`.\n",
    "\n",
    "**Synthetic Data:** Uses the FitzHugh-Nagumo model with spatially delayed copies to create spatio-temporal data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pysindy as ps\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from scipy.integrate import solve_ivp\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Local modules\n",
    "import sindy\n",
    "import sindy_shred_net\n",
    "import plotting\n",
    "from utils import get_device, TimeSeriesDataset\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Create results directory\n",
    "RESULTS_DIR = \"results/synthetic_data\"\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "print(f\"Results will be saved to: {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting configuration\n",
    "sns.set_context(\"paper\")\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "pcolor_kwargs = {\n",
    "    \"vmin\": -3,\n",
    "    \"vmax\": 3,\n",
    "    \"cmap\": \"RdBu_r\",\n",
    "    \"rasterized\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "device-header",
   "metadata": {},
   "source": [
    "### Device and Seed Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "device-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device selection\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "SEED = 0\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-header",
   "metadata": {},
   "source": [
    "## 2. Data Generation\n",
    "\n",
    "Generate synthetic spatio-temporal data from the FitzHugh-Nagumo model with spatially delayed copies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dynamics-def",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rhs_FNM(t, x, tau, a, b, Iext):\n",
    "    \"\"\"FitzHugh-Nagumo Model.\"\"\"\n",
    "    v, w = x\n",
    "    vdot = v - (v**3) / 3 - w + Iext\n",
    "    wdot = (1 / tau) * (v + a - b * w)\n",
    "    return np.array([vdot, wdot])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time integration parameters\n",
    "T = 64\n",
    "dt_solve = 0.0001 * 8\n",
    "t_solution = np.arange(0, T, dt_solve)\n",
    "\n",
    "# FitzHugh-Nagumo parameters\n",
    "x0 = np.array([-1.110, -0.125])\n",
    "tau1 = 2\n",
    "a = 0.7\n",
    "b = 0.8\n",
    "Iext = 0.65\n",
    "\n",
    "# Solve the ODE\n",
    "solution_fn = solve_ivp(\n",
    "    rhs_FNM, [0, T], x0, t_eval=t_solution, args=(tau1, a, b, Iext)\n",
    ")\n",
    "\n",
    "print(f\"FitzHugh-Nagumo solution shape: {solution_fn.y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mix-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create spatially delayed copies\n",
    "num_space_dims = 10\n",
    "delays = np.linspace(0, 2, num_space_dims)  # delays in time units\n",
    "uv_spatial = np.zeros((len(t_solution), 2 * num_space_dims))\n",
    "\n",
    "for i in range(num_space_dims):\n",
    "    delay_steps = int(delays[i] / dt_solve)\n",
    "    if delay_steps == 0:\n",
    "        uv_spatial[:, 2 * i : 2 * i + 2] = solution_fn.y.T\n",
    "    else:\n",
    "        # Pad with initial condition and shift\n",
    "        uv_spatial[:, 2 * i : 2 * i + 2] = np.vstack(\n",
    "            [np.tile(x0, (delay_steps, 1)), solution_fn.y.T[:-delay_steps, :]]\n",
    "        )\n",
    "\n",
    "# Subsample for computational efficiency\n",
    "substep = 50\n",
    "uv_spatial = uv_spatial[0::substep, :]\n",
    "t_solution = t_solution[0::substep]\n",
    "time = t_solution\n",
    "dt_data = time[1] - time[0]\n",
    "\n",
    "# Get dimensions\n",
    "n_space_dims = uv_spatial.shape[1]\n",
    "n_time = uv_spatial.shape[0]\n",
    "\n",
    "# Final data matrix (space x time for visualization, time x space for processing)\n",
    "data_original = uv_spatial.T\n",
    "\n",
    "print(f\"Data shape (space x time): {data_original.shape}\")\n",
    "print(f\"Time step: {dt_data:.6f}\")\n",
    "print(f\"Number of time samples: {n_time}\")\n",
    "print(f\"Spatial dimension: {n_space_dims}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize generated data\n",
    "space_dim = np.arange(n_space_dims)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 3))\n",
    "\n",
    "ax.pcolormesh(time, space_dim, data_original, **pcolor_kwargs)\n",
    "ax.set_title(r\"Spatio-temporal data $\\mathbf{x}$\", loc=\"left\")\n",
    "ax.set_ylabel(\"Space\")\n",
    "ax.set_xlabel(\"Time\")\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(f\"{RESULTS_DIR}/data_original.pdf\", bbox_inches=\"tight\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## 3. Configuration and Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sensor configuration (fixed for reproducibility)\n",
    "sensor_locations = np.array([5, 14, 7])\n",
    "num_sensors = len(sensor_locations)\n",
    "\n",
    "# Model hyperparameters\n",
    "latent_dim = 2\n",
    "poly_order = 3\n",
    "include_sine = False\n",
    "\n",
    "# Calculate library dimension\n",
    "library_dim = sindy.library_size(latent_dim, poly_order, include_sine, include_constant=True)\n",
    "\n",
    "# Data split configuration\n",
    "lags = 120\n",
    "train_length = 750 // 4\n",
    "validate_length = 0\n",
    "\n",
    "# Prepare data (transpose to time x space, subsample)\n",
    "load_X = copy.deepcopy(data_original)\n",
    "load_X = load_X.T[::4]  # Subsample by 4\n",
    "dt = dt_data * 4\n",
    "lags = lags // 4\n",
    "\n",
    "n = load_X.shape[0]\n",
    "m = load_X.shape[1]\n",
    "\n",
    "# SINDy threshold\n",
    "sindy_threshold = 0.10\n",
    "\n",
    "print(f\"Data shape after preprocessing: {load_X.shape}\")\n",
    "print(f\"Number of sensors: {num_sensors}\")\n",
    "print(f\"Latent dimension: {latent_dim}\")\n",
    "print(f\"Library dimension: {library_dim}\")\n",
    "print(f\"Trajectory length (lags): {lags}\")\n",
    "print(f\"Training length: {train_length}\")\n",
    "print(f\"Time step: {dt:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-sensors",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sensor time series\n",
    "t_plot = time[::4]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.plot(t_plot, load_X[:, sensor_locations], color='b', alpha=0.7)\n",
    "ax.axvline(t_plot[train_length], color='k', linestyle='--', label='Train/Test split')\n",
    "ax.axvline(t_plot[lags], color='r', linestyle=':', label='Lag window')\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Sensor value')\n",
    "ax.set_title('Sensor Time Series')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/test indices\n",
    "train_indices = np.arange(0, train_length)\n",
    "\n",
    "mask = np.ones(n - lags)\n",
    "mask[train_indices] = 0\n",
    "test_indices = np.arange(0, n - lags)[np.where(mask != 0)[0]]\n",
    "\n",
    "# For this example, validation = empty\n",
    "valid_indices = test_indices[:validate_length] if validate_length > 0 else train_indices[:1]\n",
    "\n",
    "print(f\"Train samples: {len(train_indices)}\")\n",
    "print(f\"Test samples: {len(test_indices)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scale-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data using MinMaxScaler\n",
    "sc = MinMaxScaler()\n",
    "sc = sc.fit(load_X[train_indices])\n",
    "transformed_X = sc.transform(load_X)\n",
    "\n",
    "# Generate input sequences (sensor trajectories)\n",
    "all_data_in = np.zeros((n - lags, lags, num_sensors))\n",
    "for i in range(len(all_data_in)):\n",
    "    all_data_in[i] = transformed_X[i:i+lags, sensor_locations]\n",
    "\n",
    "# Create input/output tensors\n",
    "train_data_in = torch.tensor(all_data_in[train_indices], dtype=torch.float32).to(device)\n",
    "valid_data_in = torch.tensor(all_data_in[valid_indices], dtype=torch.float32).to(device)\n",
    "test_data_in = torch.tensor(all_data_in[test_indices], dtype=torch.float32).to(device)\n",
    "\n",
    "train_data_out = torch.tensor(transformed_X[train_indices + lags - 1], dtype=torch.float32).to(device)\n",
    "valid_data_out = torch.tensor(transformed_X[valid_indices + lags - 1], dtype=torch.float32).to(device)\n",
    "test_data_out = torch.tensor(transformed_X[test_indices + lags - 1], dtype=torch.float32).to(device)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TimeSeriesDataset(train_data_in, train_data_out)\n",
    "valid_dataset = TimeSeriesDataset(valid_data_in, valid_data_out)\n",
    "test_dataset = TimeSeriesDataset(test_data_in, test_data_out)\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-header",
   "metadata": {},
   "source": [
    "## 4. Model Creation and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SINDy-SHRED network\n",
    "shred = sindy_shred_net.SINDy_SHRED_net(\n",
    "    input_size=num_sensors,\n",
    "    output_size=m,\n",
    "    hidden_size=latent_dim,\n",
    "    hidden_layers=2,\n",
    "    l1=350,\n",
    "    l2=400,\n",
    "    dropout=0.1,\n",
    "    library_dim=library_dim,\n",
    "    poly_order=poly_order,\n",
    "    include_sine=include_sine,\n",
    "    dt=dt,\n",
    ").to(device)\n",
    "\n",
    "print(\"SINDy-SHRED network created\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in shred.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "validation_errors = sindy_shred_net.fit(\n",
    "    shred,\n",
    "    train_dataset,\n",
    "    valid_dataset,\n",
    "    batch_size=64,\n",
    "    num_epochs=600,\n",
    "    lr=1e-3,\n",
    "    verbose=True,\n",
    "    threshold=0.05,\n",
    "    patience=5,\n",
    "    sindy_regularization=10.0,\n",
    "    optimizer=\"AdamW\",\n",
    "    thres_epoch=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval-header",
   "metadata": {},
   "source": [
    "## 5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval-recon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute test reconstruction error\n",
    "test_recons = sc.inverse_transform(shred(test_dataset.X).detach().cpu().numpy())\n",
    "test_ground_truth = sc.inverse_transform(test_dataset.Y.detach().cpu().numpy())\n",
    "\n",
    "relative_error = np.linalg.norm(test_recons - test_ground_truth) / np.linalg.norm(test_ground_truth)\n",
    "print(f\"Test set relative reconstruction error: {relative_error:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-recon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize reconstruction\n",
    "fig, axes = plt.subplots(2, 1, figsize=(10, 5), sharex=True)\n",
    "\n",
    "ax = axes[0]\n",
    "ax.pcolormesh(test_ground_truth.T, **pcolor_kwargs)\n",
    "ax.set_title(\"Ground Truth\")\n",
    "ax.set_ylabel(\"Space\")\n",
    "\n",
    "ax = axes[1]\n",
    "ax.pcolormesh(test_recons.T, **pcolor_kwargs)\n",
    "ax.set_title(\"SINDy-SHRED Reconstruction\")\n",
    "ax.set_ylabel(\"Space\")\n",
    "ax.set_xlabel(\"Time step\")\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(f\"{RESULTS_DIR}/reconstruction_comparison.pdf\", bbox_inches=\"tight\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sindy-header",
   "metadata": {},
   "source": [
    "## 6. Post-hoc SINDy Discovery\n",
    "\n",
    "Extract latent trajectories and discover sparse governing equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extract-latent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract latent trajectories from training data\n",
    "gru_outs_train, _ = shred.gru_outputs(train_dataset.X, sindy=True)\n",
    "gru_outs_train = gru_outs_train[:, 0, :]\n",
    "\n",
    "# Save min/max for each latent dimension (needed for denormalization later)\n",
    "latent_min = torch.min(gru_outs_train, dim=0).values\n",
    "latent_max = torch.max(gru_outs_train, dim=0).values\n",
    "\n",
    "# Normalize latent trajectories to [-1, 1]\n",
    "gru_outs_normalized = gru_outs_train.clone()\n",
    "for i in range(latent_dim):\n",
    "    gru_outs_normalized[:, i] = (gru_outs_train[:, i] - latent_min[i]) / (latent_max[i] - latent_min[i])\n",
    "gru_outs_normalized = 2 * gru_outs_normalized - 1\n",
    "\n",
    "x_train = gru_outs_normalized.detach().cpu().numpy()\n",
    "gru_outs_train_np = gru_outs_train.detach().cpu().numpy()\n",
    "print(f\"Latent trajectories shape: {x_train.shape}\")\n",
    "print(f\"Latent min: {latent_min.detach().cpu().numpy()}\")\n",
    "print(f\"Latent max: {latent_max.detach().cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sindy-discover",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SINDy discovery\n",
    "differentiation_method = ps.differentiation.FiniteDifference()\n",
    "\n",
    "model = ps.SINDy(\n",
    "    optimizer=ps.STLSQ(threshold=sindy_threshold, alpha=0.05),\n",
    "    differentiation_method=differentiation_method,\n",
    "    feature_library=ps.PolynomialLibrary(degree=poly_order),\n",
    ")\n",
    "\n",
    "model.fit(x_train, t=dt)\n",
    "print(\"\\nDiscovered SINDy equations:\")\n",
    "model.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "true-eqns-header",
   "metadata": {},
   "source": [
    "### True Governing Equations\n",
    "\n",
    "**FitzHugh-Nagumo Model:**\n",
    "$$\\dot{v} = v - \\frac{1}{3}v^3 - w + 0.65$$\n",
    "$$\\dot{w} = \\frac{1}{\\tau}(v + 0.7 - 0.8w)$$\n",
    "with $\\tau = 2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sindy-simulate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate discovered model\n",
    "t_sim = np.arange(0, len(x_train) * dt, dt)\n",
    "init_cond = x_train[0, :]\n",
    "x_sim = model.simulate(init_cond, t_sim)\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(latent_dim, 1, figsize=(10, 2 * latent_dim), sharex=True)\n",
    "for i in range(latent_dim):\n",
    "    axes[i].plot(t_sim, x_train[:len(t_sim), i], label=\"SINDy-SHRED\")\n",
    "    axes[i].plot(t_sim, x_sim[:, i], \"k--\", label=\"Identified model\")\n",
    "    axes[i].set_ylabel(rf\"$z_{{{i}}}$\")\n",
    "    if i == latent_dim - 1:\n",
    "        axes[i].set_xlabel(\"Time\")\n",
    "        axes[i].legend()\n",
    "\n",
    "fig.suptitle(\"Latent Space: SINDy-SHRED vs Identified Model\")\n",
    "fig.tight_layout()\n",
    "fig.savefig(f\"{RESULTS_DIR}/latent_comparison.pdf\", bbox_inches=\"tight\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sensor-pred-header",
   "metadata": {},
   "source": [
    "## 7. Sensor-Level Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sensor-pred",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test latent from GRU\n",
    "gru_outs_test, _ = shred.gru_outputs(test_dataset.X, sindy=True)\n",
    "gru_outs_test = gru_outs_test[:, 0, :]\n",
    "\n",
    "# Normalize test latent using TRAINING min/max\n",
    "test_normalized = gru_outs_test.clone()\n",
    "for i in range(latent_dim):\n",
    "    test_normalized[:, i] = (gru_outs_test[:, i] - latent_min[i]) / (latent_max[i] - latent_min[i])\n",
    "test_normalized = 2 * test_normalized - 1\n",
    "test_normalized_np = test_normalized.detach().cpu().numpy()\n",
    "\n",
    "# Use first test point as initial condition and simulate SINDy forward\n",
    "t_test = np.arange(0, len(test_normalized_np) * dt, dt)\n",
    "init_cond_test = test_normalized_np[0, :]\n",
    "x_sindy_test = model.simulate(init_cond_test, t_test)\n",
    "\n",
    "# Denormalize SINDy output back to original latent scale\n",
    "# Reverse of: normalized = 2 * (x - min) / (max - min) - 1\n",
    "# So: x = (normalized + 1) / 2 * (max - min) + min\n",
    "x_sindy_denorm = np.zeros_like(x_sindy_test)\n",
    "for i in range(latent_dim):\n",
    "    x_sindy_denorm[:, i] = (x_sindy_test[:, i] + 1) / 2 * (latent_max[i].item() - latent_min[i].item()) + latent_min[i].item()\n",
    "\n",
    "# Decode through SDN (decoder only) to get physical space prediction\n",
    "x_sindy_tensor = torch.tensor(x_sindy_denorm, dtype=torch.float32).to(device)\n",
    "sindy_physical_scaled = shred.decode(x_sindy_tensor).detach().cpu().numpy()\n",
    "\n",
    "# Inverse transform to original data scale\n",
    "sindy_physical = sc.inverse_transform(sindy_physical_scaled)\n",
    "\n",
    "print(f\"SINDy latent prediction shape: {x_sindy_test.shape}\")\n",
    "print(f\"Decoded physical prediction shape: {sindy_physical.shape}\")\n",
    "\n",
    "# Plot sensor-level comparisons: Ground Truth vs SINDy Prediction\n",
    "fig, axes = plotting.plot_sensor_predictions(\n",
    "    test_ground_truth,\n",
    "    sindy_physical[:len(test_ground_truth)],\n",
    "    sensor_locations=np.arange(n_space_dims),  # All spatial dims\n",
    "    rows=2,\n",
    "    cols=5,\n",
    "    save_path=f\"{RESULTS_DIR}/sensor_predictions_grid.pdf\"\n",
    ")\n",
    "fig.suptitle(\"Sensor-Level: Ground Truth vs SINDy Prediction\")\n",
    "fig.tight_layout()\n",
    "print(f\"Saved sensor predictions plot to {RESULTS_DIR}/sensor_predictions_grid.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-header",
   "metadata": {},
   "source": [
    "## 8. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model\n",
    "torch.save(shred.state_dict(), f\"{RESULTS_DIR}/shred_model.pt\")\n",
    "print(f\"Saved SHRED model to {RESULTS_DIR}/shred_model.pt\")\n",
    "\n",
    "# Save latent trajectories\n",
    "np.save(f\"{RESULTS_DIR}/latent_train.npy\", x_train)\n",
    "print(f\"Saved latent trajectories\")\n",
    "\n",
    "# Save SINDy coefficients\n",
    "sindy_coefficients = model.coefficients()\n",
    "np.save(f\"{RESULTS_DIR}/sindy_coefficients.npy\", sindy_coefficients)\n",
    "print(f\"Saved SINDy coefficients: shape {sindy_coefficients.shape}\")\n",
    "\n",
    "# Save feature names\n",
    "feature_names = model.get_feature_names()\n",
    "with open(f\"{RESULTS_DIR}/sindy_feature_names.txt\", \"w\") as f:\n",
    "    for name in feature_names:\n",
    "        f.write(name + \"\\n\")\n",
    "\n",
    "# Save original data\n",
    "np.save(f\"{RESULTS_DIR}/data_original.npy\", data_original)\n",
    "\n",
    "# Save config\n",
    "config = {\n",
    "    \"latent_dim\": latent_dim,\n",
    "    \"poly_order\": poly_order,\n",
    "    \"num_sensors\": num_sensors,\n",
    "    \"lags\": lags,\n",
    "    \"train_length\": train_length,\n",
    "    \"dt\": dt,\n",
    "    \"sindy_threshold\": sindy_threshold,\n",
    "    \"relative_error\": relative_error,\n",
    "}\n",
    "np.save(f\"{RESULTS_DIR}/config.npy\", config)\n",
    "\n",
    "print(\"\\nAll results saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
