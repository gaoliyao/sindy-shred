{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functionalizing the SINDy-SHRED pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out SINDy-SHRED fits for ENSO signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import driver as model_driver\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pysindy as ps\n",
    "\n",
    "# netcdf/numpy/xray/stats\n",
    "import scipy\n",
    "\n",
    "# import plotting\n",
    "import seaborn as sns\n",
    "import sindy\n",
    "import sindy_shred\n",
    "import torch\n",
    "import xarray as xr\n",
    "from pysindy.differentiation import FiniteDifference\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust these to your paths\n",
    "dir_hadisst_data = os.path.join(\n",
    "    \"/Users/karllapo/Library/CloudStorage/Dropbox/ESPRIT/proj/data/climate/HadISST/\"\n",
    ")\n",
    "dir_print = os.path.join(\n",
    "    \"/Users/karllapo/Library/CloudStorage/Dropbox/ESPRIT/proj/notebooks/sindy-shred/sindy-shred/figures/HadISST\"\n",
    ")\n",
    "dir_results = os.path.join(\n",
    "    \"/Users/karllapo/Dropbox/ESPRIT/proj/notebooks/mrdmd/PNAS figures/results/HadISST\"\n",
    ")\n",
    "\n",
    "# Load the HadISST data\n",
    "ds_obs = xr.open_dataset(\n",
    "    os.path.join(dir_hadisst_data, \"HadISST_sst.analysis-subset.nc\"), engine=\"h5netcdf\"\n",
    ")\n",
    "data_name = \"HadISST-subset\"\n",
    "\n",
    "t1 = pd.Timestamp(ds_obs.time.values[0])\n",
    "t1 = t1.replace(hour=0)\n",
    "t1 = t1.replace(second=0)\n",
    "t1 = t1.replace(minute=0)\n",
    "t1 = t1.replace(microsecond=0)\n",
    "\n",
    "t2 = pd.Timestamp(ds_obs.time.values[-1])\n",
    "t2 = t2.replace(hour=0)\n",
    "t2 = t2.replace(second=0)\n",
    "t2 = t2.replace(minute=0)\n",
    "t2 = t2.replace(microsecond=0)\n",
    "\n",
    "# Create an even 30 day sample instead of monthly\n",
    "even_time = pd.date_range(start=t1, end=t2, freq=\"30d\")\n",
    "ds_obs = ds_obs.interp(time=even_time)\n",
    "\n",
    "# The interpolation causes the first time step to be dropped due to being out of the time bounds.\n",
    "ds_obs = ds_obs.dropna(how=\"all\", dim=\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define CRS's for our use case\n",
    "crs0 = ccrs.PlateCarree(\n",
    "    central_longitude=0\n",
    ")  # for coding data, same as ccrs.PlateCarree()\n",
    "crs180 = ccrs.PlateCarree(central_longitude=180)  # for plotting map in pacific area\n",
    "\n",
    "# For all plotting, use `crs180`\n",
    "fig, paco_region = plt.subplots(subplot_kw={\"projection\": crs180})\n",
    "\n",
    "p = ds_obs.sst.isel(time=1000).plot(\n",
    "    ax=paco_region,\n",
    "    transform=crs0,  # the data's projection\n",
    ")\n",
    "paco_region.gridlines(crs=crs0, draw_labels=True)\n",
    "paco_region.set_extent([120, 260, -30, 30], crs=crs0)\n",
    "paco_region.add_feature(cfeature.COASTLINE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the properties of a single snapshot, specifically the portion of the data not masked by land.\n",
    "data_snapshot = ds_obs.sst.isel(time=0).values\n",
    "ind_lat, ind_lon = np.where(~np.isnan(data_snapshot))\n",
    "n_space = len(ind_lat)\n",
    "n_time = len(ds_obs.time)\n",
    "\n",
    "longitudes_ragged = ds_obs.longitude.isel(longitude=ind_lon)\n",
    "latitudes_ragged = ds_obs.latitude.isel(latitude=ind_lat)\n",
    "\n",
    "ds_obs[\"sst\"] = ds_obs.sst.transpose(\"longitude\", \"latitude\", \"time\")\n",
    "data_3d = ds_obs.sst.values\n",
    "\n",
    "data_1d = data_3d.reshape(\n",
    "    ds_obs.longitude.size * ds_obs.latitude.size, ds_obs.time.size\n",
    ")\n",
    "nandex_1d = np.nonzero(np.isnan(data_1d))\n",
    "notnandex_1d = np.nonzero(~np.isnan(data_1d))\n",
    "\n",
    "data_to_fit = data_1d[notnandex_1d]\n",
    "data_to_fit = data_to_fit.reshape(n_space, n_time)\n",
    "\n",
    "print(data_to_fit.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert time to days\n",
    "ts = ds_obs.time\n",
    "ts = ts - ts.isel(time=0)\n",
    "ts = (ts / (1e9 * 24 * 60 * 60)).values.astype(float)\n",
    "# Time step in days\n",
    "dt = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enso_box_extract(ds, box):\n",
    "    \"\"\"\n",
    "    Extract out data within the ENSO 3.4 box.\n",
    "    \"\"\"\n",
    "    ds_sub = ds.where(\n",
    "        (ds.latitude > box[2])\n",
    "        & (ds.latitude < box[3])\n",
    "        & (ds.longitude < box[1])\n",
    "        & (ds.longitude > box[0]),\n",
    "        drop=True,\n",
    "    )\n",
    "\n",
    "    return ds_sub\n",
    "\n",
    "\n",
    "def xarray_unstacked(x, time, longitudes, latitudes):\n",
    "    \"\"\"\n",
    "    Unstack data back to longitude by latitude.\n",
    "    \"\"\"\n",
    "    ds_unstacked = xr.DataArray(\n",
    "        x,\n",
    "        coords={\"time\": time},\n",
    "        dims=[\"xy\", \"time\"],\n",
    "    )\n",
    "\n",
    "    ds_unstacked.coords[\"longitude\"] = (\"xy\", longitudes.data)\n",
    "    ds_unstacked.coords[\"latitude\"] = (\"xy\", latitudes.data)\n",
    "    ds_unstacked = ds_unstacked.set_index(xy=(\"latitude\", \"longitude\")).unstack(\"xy\")\n",
    "\n",
    "    return ds_unstacked\n",
    "\n",
    "\n",
    "def longitude_reorder(ds):\n",
    "    \"\"\"\n",
    "    Help handle the annoying zero-line crossing that perplexes Cartopy.\n",
    "    \"\"\"\n",
    "    ds[\"longitude\"] = xr.where(\n",
    "        ds.longitude > 0,\n",
    "        ds.longitude - 180,\n",
    "        ds.longitude + 180,\n",
    "    )\n",
    "    ds = ds.sortby(ds.longitude)\n",
    "    return ds\n",
    "\n",
    "\n",
    "def monthly_anomaly(ds, modern_baseline=True):\n",
    "    \"\"\"\n",
    "    Build monthly anomalies to be consistent with the definition of ENSO3.4\n",
    "    \"\"\"\n",
    "\n",
    "    if modern_baseline:\n",
    "        monthly_mean = (\n",
    "            ds.sel(time=slice(\"1991-01-01\", \"2020-01-01\")).groupby(\"time.month\").mean()\n",
    "        )\n",
    "\n",
    "    ds_monthly_anomaly = ds.groupby(\"time.month\") - monthly_mean\n",
    "    ds_monthly_anomaly = ds_monthly_anomaly.rolling(time=3, center=True).mean()\n",
    "\n",
    "    return ds_monthly_anomaly\n",
    "\n",
    "\n",
    "def plot_enso_box(ax, enso_box):\n",
    "    \"\"\"\n",
    "    Plot the ENSO 3.4 box on a cartopy map.\n",
    "    \"\"\"\n",
    "    ax.add_patch(\n",
    "        mpatches.Rectangle(\n",
    "            xy=[enso_box[0], enso_box[2]],\n",
    "            width=50,\n",
    "            height=10,\n",
    "            facecolor=\"none\",\n",
    "            edgecolor=\"k\",\n",
    "            transform=crs180,\n",
    "            zorder=10,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "def add_grid_lines(ax, no_bottom_labels=False, label_kwargs=None):\n",
    "    \"\"\"\n",
    "    Add grid lines to a cartopy map.\n",
    "    \"\"\"\n",
    "\n",
    "    if label_kwargs is None:\n",
    "        label_kwargs = {\"fontsize\": 7}\n",
    "\n",
    "    gl = ax.gridlines(crs=crs180, draw_labels=True)\n",
    "    if not ax.axes.get_subplotspec().is_first_col():\n",
    "        gl.left_labels = False\n",
    "    if no_bottom_labels:\n",
    "        gl.bottom_labels = False\n",
    "    gl.top_labels = False\n",
    "    gl.right_labels = False\n",
    "    gl.xlines = False\n",
    "    gl.ylines = False\n",
    "    gl.xlabel_style = label_kwargs\n",
    "    gl.ylabel_style = label_kwargs\n",
    "\n",
    "    ax.set_extent([120, 260, -30, 30], crs=crs0)\n",
    "    ax.add_feature(cfeature.COASTLINE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Oceanic El Nino Index (ONI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enso34_box = [-170, -120, -5, 5]\n",
    "\n",
    "ds_obs_enso = enso_box_extract(ds_obs.sst, enso34_box)\n",
    "ds_reference = ds_obs_enso.sel(\n",
    "    time=slice(pd.Timestamp(\"1991-01-01\"), pd.Timestamp(\"2020-12-31\"))\n",
    ")\n",
    "reference_baseline = (\n",
    "    ds_reference.mean(dim=[\"latitude\", \"longitude\"]).groupby(\"time.month\").mean()\n",
    ")\n",
    "\n",
    "background_total_mean = ds_obs_enso.mean(dim=[\"latitude\", \"longitude\"])\n",
    "background_total_mean = (\n",
    "    background_total_mean.sel(time=slice(\"1991-01-01\", \"2020-12-31\"))\n",
    "    .groupby(\"time.month\")\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "background_spatial_mean = ds_obs_enso.mean(dim=[\"latitude\", \"longitude\"]).groupby(\n",
    "    \"time.month\"\n",
    ")\n",
    "background_anomaly = background_spatial_mean - background_total_mean\n",
    "\n",
    "# This is now the ONI\n",
    "background_anomaly = background_anomaly.rolling(time=3, center=True).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_monthly = ds_obs_enso.groupby(\"time.month\")\n",
    "background_total_mean = ds_reference.mean(dim=[\"latitude\", \"longitude\"])\n",
    "background_total_mean = background_total_mean.groupby(\"time.month\").mean()\n",
    "\n",
    "# This is the ONI but without the spatial averaging\n",
    "ds_oni = background_monthly - background_total_mean\n",
    "ds_oni = ds_oni.rolling(time=3, center=True).mean().dropna(dim=\"time\", how=\"any\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an ONI for fitting in SINDy-SHRED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oni_snapshot = ds_oni.isel(time=0).values\n",
    "ind_lon, ind_lat = np.where(~np.isnan(oni_snapshot))\n",
    "n_space_oni = len(ind_lat)\n",
    "n_time_oni = len(ds_oni.time)\n",
    "\n",
    "lon_ragged_oni = ds_oni.longitude.isel(longitude=ind_lon)\n",
    "lat_ragged_oni = ds_oni.latitude.isel(latitude=ind_lat)\n",
    "\n",
    "oni_3d = ds_oni.values\n",
    "\n",
    "oni_1d = oni_3d.reshape(ds_oni.longitude.size * ds_oni.latitude.size, ds_oni.time.size)\n",
    "nandex_1d = np.nonzero(np.isnan(oni_1d))\n",
    "notnandex_1d = np.nonzero(~np.isnan(oni_1d))\n",
    "\n",
    "oni_to_fit = oni_1d[notnandex_1d]\n",
    "oni_to_fit = oni_to_fit.reshape(n_space_oni, n_time_oni)\n",
    "\n",
    "print(oni_to_fit.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale-aware SINDy-SHRED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this BEFORE importing torch if you need to control GPU access\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "# Choose device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Optional: CUDA-specific seed (only if using CUDA)\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.manual_seed(0)\n",
    "\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now select indices to divide the data into training, validation, and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SINDy-SHRED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify data to fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modes_to_fit = \"ONI\"\n",
    "\n",
    "# We will select a subset of the data, in this case starting in the modern period.\n",
    "# t_ind = slice(80 * 12, None)  # From ~1950 to present\n",
    "t_ind = slice(128 * 12, None)  # From ~2006 to present\n",
    "\n",
    "if modes_to_fit == \"original\":\n",
    "    lags = np.round(365 / dt).astype(int)\n",
    "    scale_aware_data = data_to_fit[:, t_ind]\n",
    "    num_train_years = 10\n",
    "    num_test_years = 5\n",
    "    num_validate_years = 0.5\n",
    "\n",
    "elif modes_to_fit == \"ONI\":\n",
    "    lags = 24\n",
    "    scale_aware_data = oni_to_fit[:, t_ind]\n",
    "    num_train_years = 10\n",
    "    num_test_years = 5\n",
    "    num_validate_years = 0.01\n",
    "\n",
    "ts_sindy_shred = ts[t_ind]\n",
    "ts_physical = ds_obs.time[t_ind]\n",
    "\n",
    "num_sensors = 500\n",
    "load_X = scale_aware_data.T\n",
    "len_t = load_X.shape[0]  # time dimension length\n",
    "len_space = load_X.shape[1]  # space dimension length\n",
    "\n",
    "sensor_locations = np.random.choice(len_space, size=num_sensors, replace=False)\n",
    "\n",
    "train_length = np.round(num_train_years * 365 / dt).astype(int)\n",
    "validate_length = np.round(num_validate_years * 365 / dt).astype(int)\n",
    "test_length = np.round(num_test_years * 365 / dt).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the data and data splits to make sure they make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = scale_aware_data.shape[0]\n",
    "for n in [0, s // 4, s // 2, int(s // 4 * 3)]:\n",
    "    plt.plot(scale_aware_data[n], color=\"k\")\n",
    "\n",
    "plt.plot(scale_aware_data.mean(axis=0), color=\"r\", lw=2)\n",
    "plt.gca().axvline(train_length)\n",
    "plt.gca().axvline(validate_length + train_length)\n",
    "plt.gca().axvline(lags)\n",
    "plt.xlim(0, train_length + validate_length + test_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the library.\n",
    "\n",
    "Latent dim of 12 leads to a great reconstruction of the splits. But! Posthoc model discovery doesn't seem to work very well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 12\n",
    "poly_order = 1\n",
    "include_sine = False\n",
    "library_dim = sindy.library_size(latent_dim, poly_order, include_sine, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "driver = model_driver.sindy_shred_driver(\n",
    "    latent_dim=latent_dim,\n",
    "    poly_order=poly_order,\n",
    "    verbose=True,\n",
    "    # It really needs the extra training time from 2000\n",
    "    num_epochs=1000,\n",
    "    threshold=0.000,\n",
    "    sample_mode=\"forecast\",\n",
    "    batch_size=32,\n",
    "    sindy_regularization=1.0,\n",
    ")\n",
    "driver.fit(\n",
    "    num_sensors,\n",
    "    dt,\n",
    "    load_X,\n",
    "    lags,\n",
    "    train_length,\n",
    "    validate_length,\n",
    "    sensor_locations,\n",
    "    test_length=test_length,\n",
    "    seed=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posthoc model discovery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block extracts latent states using a GRU model. Each of the first three dimensions is normalized and re-scaled to [-1,1]. A SINDy model is then set up with a polynomial library. This SINDy model is fitted to the data and later used to simulate the dynamics for comparison with the original GRU latent trajectories.\n",
    "\n",
    "The hyperparameters for the SINDy model are set as follows:\n",
    "- **Differentiation Method:** Finite difference is used to compute numerical derivatives. When the latent states trajectories are noisy, one should consider to use ps.differentiation.SmoothedFiniteDifference().\n",
    "- **Optimizer Sequantially thresholded least-squares: STLSQ:**  \n",
    "  - *Threshold:* 0.8, which controls the sparsity level by eliminating coefficients below this value.\n",
    "  - *Alpha:* 0.05, L2 regularization.\n",
    "- **Optimizer Mixed-interger optimization: MIOSR:**  \n",
    "  - *group_sparsity:* how many terms each equations should include. \n",
    "- **Feature Library:** Here we only include polynomial features up to degree 1, ensuring only linear terms are considered in the model. Practically this could set up to degree 3 which will include more nonlinear terms. \n",
    "\n",
    "For model selection, a simple baseline standard is to visually examine how the long-term extrapolation behaves. We can pick the model that fits the data well. It is also possible to include metrics (like the MSE) to quantitatively measure these quantities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# differentiation_method = \"finite\"\n",
    "# if differentiation_method == \"finite\":\n",
    "#     driver._differentiation_method = ps.differentiation.FiniteDifference()\n",
    "# elif differentiation_method == \"smoothed finite\":\n",
    "#     driver._differentiation_method = ps.differentiation.SmoothedFiniteDifference()\n",
    "\n",
    "# gru_outs = driver.gru_normalize(data_type=\"train\")\n",
    "# x = gru_outs.detach().cpu().numpy()\n",
    "\n",
    "# gru_outs = driver.gru_normalize(data_type=\"train\")\n",
    "\n",
    "# # SINDy discovery\n",
    "# x = gru_outs.detach().cpu().numpy()\n",
    "# driver._gru_outs = x\n",
    "\n",
    "# # model = ps.SINDy(\n",
    "# #     optimizer=ps.STLSQ(threshold=threshold, alpha=0.05),\n",
    "# #     differentiation_method=driver._differentiation_method,\n",
    "# #     feature_library=ps.PolynomialLibrary(degree=driver._poly_order),\n",
    "# # )\n",
    "\n",
    "# model = ps.SINDy(\n",
    "#     optimizer=ps.MIOSR(group_sparsity=np.ones(driver._latent_dim) * 10, alpha=5000),\n",
    "#     differentiation_method=driver._differentiation_method,\n",
    "#     feature_library=ps.PolynomialLibrary(degree=poly_order),\n",
    "# )\n",
    "\n",
    "# model.fit(x, t=driver._dt, ensemble=True)\n",
    "# driver._model = model\n",
    "\n",
    "# print(\"SINDy-derived dynamical equation:\\n\")\n",
    "# model.print()\n",
    "\n",
    "# driver.sindy_simulate(x)\n",
    "# x_sim = driver._x_sim\n",
    "# t_train = np.arange(0, len(x) * driver._dt, driver._dt)\n",
    "# fig, ax = plt.subplots(driver._latent_dim, sharex=True, sharey=True)\n",
    "# for i in range(driver._latent_dim):\n",
    "#     ax[i].plot(\n",
    "#         t_train, gru_outs[:, i].detach().cpu().numpy(), label=\"SINDy-SHRED\"\n",
    "#     )\n",
    "#     ax[i].plot(t_train, x_sim[:, i], \"k--\", label=\"identified model\")\n",
    "#     ax[i].set_ylabel(rf\"$z_{{{i}}}$ (-)\")\n",
    "#     if i == driver._latent_dim - 1:\n",
    "#         ax[i].set_xlabel(\"time (s)\")\n",
    "#         ax[i].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(driver._model.coefficients().flatten(), bins=np.arange(-0.03, 0.03, 0.001));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.pcolormesh(driver._model.coefficients(), vmin=-0.03, vmax=0.03, cmap=\"RdBu_r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for threshold in [0]: #[10**-6, 10**-5, 10**-4, 10**-3]:\n",
    "    driver.sindy_identify(\n",
    "        threshold=threshold, plot_result=True, differentiation_method=\"finite\"\n",
    "    )\n",
    "    plt.gcf().suptitle(f\"latent space for {modes_to_fit} mode\")\n",
    "    plt.gcf().tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for threshold in [10**-6, 10**-5, 10**-4, 10**-3]:\n",
    "# for threshold in [0]:\n",
    "#     driver.sindy_identify(th|reshold=threshold, plot_result=True)\n",
    "#     plt.gcf().suptitle(f\"latent space for {modes_to_fit} mode\")\n",
    "#     plt.gcf().tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take coefficients and construct rxr matrix. Nathan's hypothesis: r eigenvalues with r/2 complex conjugate pairs (and maybe one DC component if r is odd). This tests if we have r/2 oscillating frequencies, as we would expect for a Koopman case. Potentially allow us to interpret these terms in place of the posthoc discovery and that we have a small number of linear oscillators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Validate-Test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_recons = driver._scaler.inverse_transform(\n",
    "    driver._shred(driver._train_data.X).detach().cpu().numpy()\n",
    ")\n",
    "train_ground_truth = driver._scaler.inverse_transform(\n",
    "    driver._train_data.Y.detach().cpu().numpy()\n",
    ")\n",
    "print(\n",
    "    np.linalg.norm(train_recons - train_ground_truth)\n",
    "    / np.linalg.norm(train_ground_truth)\n",
    ")\n",
    "train_recons.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in [0, s // 4, s // 2, int(s // 4 * 3)]:\n",
    "    plt.plot(train_recons[:, n], color=\"k\")\n",
    "    plt.plot(train_ground_truth[:, n], color=\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_recons = driver._scaler.inverse_transform(\n",
    "#     driver._shred(driver._valid_data.X).detach().cpu().numpy()\n",
    "# )\n",
    "# val_ground_truth = driver._scaler.inverse_transform(\n",
    "#     driver._valid_data.Y.detach().cpu().numpy()\n",
    "# )\n",
    "# print(np.linalg.norm(val_recons - val_ground_truth) / np.linalg.norm(val_ground_truth))\n",
    "# val_recons.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for n in [0, s // 4, s // 2, int(s // 4 * 3)]:\n",
    "#     plt.plot(val_recons[:, n], color=\"k\")\n",
    "#     plt.plot(val_ground_truth[:, n], color=\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_recons = driver._scaler.inverse_transform(\n",
    "    driver._shred(driver._test_data.X).detach().cpu().numpy()\n",
    ")\n",
    "test_ground_truth = driver._scaler.inverse_transform(\n",
    "    driver._test_data.Y.detach().cpu().numpy()\n",
    ")\n",
    "print(\"error for 1 year out of sample reconstruction\")\n",
    "print(\n",
    "    np.linalg.norm(test_recons[0:12] - test_ground_truth[0:12])\n",
    "    / np.linalg.norm(test_ground_truth[0:12])\n",
    ")\n",
    "test_recons.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in [0, s // 4, s // 2, int(s // 4 * 3)]:\n",
    "    plt.plot(test_recons[:, n], color=\"k\")\n",
    "    plt.plot(test_ground_truth[:, n], color=\"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare in ENSO34 Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if modes_to_fit == \"ONI\":\n",
    "\n",
    "    time_results = ds_oni.time.values[t_ind]\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "\n",
    "    # Train\n",
    "    ds_results = xarray_unstacked(\n",
    "        train_recons.T,\n",
    "        time_results[lags : train_length + lags],\n",
    "        lon_ragged_oni,\n",
    "        lat_ragged_oni,\n",
    "    )\n",
    "    enso_component = enso_box_extract(ds_results, enso34_box)\n",
    "    enso_spatial_mean = enso_component.mean(dim=[\"longitude\", \"latitude\"])\n",
    "    plt.plot(\n",
    "        time_results[lags - 1 : train_length + lags - 1],\n",
    "        enso_spatial_mean,\n",
    "        color=\"xkcd:blue\",\n",
    "        label=\"fit\",\n",
    "    )\n",
    "\n",
    "    # Validate\n",
    "#     t = time_results[\n",
    "#         lags + train_length - 1 : lags + train_length + validate_length - 1\n",
    "#     ]\n",
    "\n",
    "#     ds_results = xarray_unstacked(\n",
    "#         val_recons.T,\n",
    "#         t,\n",
    "#         lon_ragged_oni,\n",
    "#         lat_ragged_oni,\n",
    "#     )\n",
    "#     enso_component = enso_box_extract(ds_results, enso34_box)\n",
    "#     enso_spatial_mean = enso_component.mean(dim=[\"longitude\", \"latitude\"])\n",
    "#     plt.plot(\n",
    "#         t,\n",
    "#         enso_spatial_mean,\n",
    "#         color=\"xkcd:blue\",\n",
    "#         ls=\"--\",\n",
    "#     )\n",
    "\n",
    "    # Test\n",
    "    t = time_results[\n",
    "        lags\n",
    "        + train_length\n",
    "        + validate_length\n",
    "        - 2 : lags\n",
    "        + train_length\n",
    "        + validate_length\n",
    "        + test_length\n",
    "        - 2\n",
    "    ]\n",
    "    ds_results = xarray_unstacked(\n",
    "        test_recons.T,\n",
    "        t,\n",
    "        lon_ragged_oni,\n",
    "        lat_ragged_oni,\n",
    "    )\n",
    "    enso_component = enso_box_extract(ds_results, enso34_box)\n",
    "    enso_spatial_mean = enso_component.mean(dim=[\"longitude\", \"latitude\"])\n",
    "    plt.plot(\n",
    "        t,\n",
    "        enso_spatial_mean,\n",
    "        color=\"xkcd:blue\",\n",
    "        ls=\":\",\n",
    "    )\n",
    "\n",
    "    # Observations\n",
    "    plt.plot(\n",
    "        background_anomaly.time, background_anomaly, color=\"k\", label=\"ENSO34 Index\"\n",
    "    )\n",
    "\n",
    "    plt.xlim(\n",
    "        time_results[lags],\n",
    "        time_results[train_length + validate_length + test_length + lags],\n",
    "    )\n",
    "    plt.xlim(time_results[lags], None)\n",
    "    plt.legend(ncols=3)\n",
    "    plt.ylabel(\"ENSO 34 box anomaly ($^{\\circ}C$)\")\n",
    "    plt.gca().axvline(time_results[train_length + lags - 2], ls=\"--\", color=\"0.5\")\n",
    "    plt.gca().axvline(\n",
    "        time_results[train_length + validate_length + lags - 2], ls=\"--\", color=\"0.5\"\n",
    "    )\n",
    "    plt.title(f\"fit {modes_to_fit} data\\n Comparison in ENSO34 box\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not modes_to_fit == \"ONI\":\n",
    "    time_results = ds_obs.time.values[t_ind]\n",
    "\n",
    "    plt.figure(figsize=(6, 3))\n",
    "\n",
    "    # Train\n",
    "    ds_results = xarray_unstacked(\n",
    "        train_recons.T,\n",
    "        time_results[lags : train_length + lags],\n",
    "        longitudes_ragged,\n",
    "        latitudes_ragged,\n",
    "    )\n",
    "    enso_component = enso_box_extract(ds_results, enso34_box)\n",
    "    enso_spatial_mean = enso_component.mean(dim=[\"longitude\", \"latitude\"])\n",
    "    plt.plot(\n",
    "        time_results[lags : train_length + lags],\n",
    "        enso_spatial_mean - enso_spatial_mean.mean(dim=\"time\"),\n",
    "        color=\"xkcd:blue\",\n",
    "        label=\"SINDy-SHRED\",\n",
    "    )\n",
    "\n",
    "    # Validate\n",
    "    ds_results = xarray_unstacked(\n",
    "        val_recons.T,\n",
    "        time_results[lags + train_length : lags + train_length + validate_length],\n",
    "        longitudes_ragged,\n",
    "        latitudes_ragged,\n",
    "    )\n",
    "    enso_component = enso_box_extract(ds_results, enso34_box)\n",
    "    # enso_spatial_mean = enso_component.mean(dim=[\"longitude\", \"latitude\"])\n",
    "    plt.plot(\n",
    "        time_results[lags + train_length : lags + train_length + validate_length],\n",
    "        enso_spatial_mean - enso_spatial_mean.mean(dim=\"time\"),\n",
    "        color=\"xkcd:blue\",\n",
    "        ls=\"--\",\n",
    "    )\n",
    "\n",
    "    # Test\n",
    "    t = time_results[\n",
    "        lags\n",
    "        + train_length\n",
    "        + validate_length : lags\n",
    "        + train_length\n",
    "        + validate_length\n",
    "        + test_length\n",
    "    ]\n",
    "    ds_results = xarray_unstacked(\n",
    "        test_recons.T,\n",
    "        t,\n",
    "        longitudes_ragged,\n",
    "        latitudes_ragged,\n",
    "    )\n",
    "    enso_component = enso_box_extract(ds_results, enso34_box)\n",
    "    # enso_spatial_mean = enso_component.mean(dim=[\"longitude\", \"latitude\"])\n",
    "    plt.plot(\n",
    "        t,\n",
    "        enso_spatial_mean - enso_spatial_mean.mean(dim=\"time\"),\n",
    "        color=\"xkcd:blue\",\n",
    "        ls=\":\",\n",
    "    )\n",
    "\n",
    "    # Observations\n",
    "    enso_component = enso_box_extract(ds_obs.sst, enso34_box)\n",
    "    enso_spatial_mean = enso_component.mean(dim=[\"longitude\", \"latitude\"])\n",
    "    plt.plot(\n",
    "        ds_obs.time,\n",
    "        enso_spatial_mean - enso_spatial_mean.mean(dim=\"time\"),\n",
    "        label=\"Obs\",\n",
    "        color=\"red\",\n",
    "    )\n",
    "    plt.plot(\n",
    "        background_anomaly.time, background_anomaly, color=\"k\", label=\"ENSO34 Index\"\n",
    "    )\n",
    "\n",
    "    plt.xlim(\n",
    "        time_results[lags],\n",
    "        time_results[train_length + validate_length + test_length + lags],\n",
    "    )\n",
    "    plt.legend(ncols=3)\n",
    "    plt.ylabel(\"ENSO 34 box anomaly ($^{\\circ}C$)\")\n",
    "    plt.gca().axvline(time_results[test_length], ls=\"--\", color=\"0.5\")\n",
    "    plt.gca().axvline(time_results[test_length + validate_length], ls=\"--\", color=\"0.5\")\n",
    "    plt.title(f\"fit {modes_to_fit} data\\n Comparison in ENSO34 box\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Spatial Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_results.sel(time=\"2009-11-03\").plot()\n",
    "\n",
    "plt.figure()\n",
    "ds_oni.sel(time=\"2009-11-03\").T.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    background_anomaly.sel(time=slice(ds_results.time[0], ds_results.time[-1])),\n",
    "    ds_results.mean(dim=[\"latitude\", \"longitude\"]),\n",
    ")\n",
    "plt.plot([-2, 3], [-2, 3], ls=\"--\")\n",
    "plt.ylabel(\"Predict. Anomaly (C)\")\n",
    "plt.xlabel(\"Obs. Anomaly (C)\")\n",
    "\n",
    "r = scipy.stats.pearsonr(\n",
    "    background_anomaly.sel(time=slice(ds_results.time[0], ds_results.time[-1])),\n",
    "    ds_results.mean(dim=[\"latitude\", \"longitude\"]),\n",
    ")\n",
    "print(r.correlation**2)\n",
    "dt_start = pd.Timestamp(ds_results.time[0].values).strftime(\"%Y-%m-%d\")\n",
    "dt_end = pd.Timestamp(ds_results.time[-1].values).strftime(\"%Y-%m-%d\")\n",
    "plt.title(\n",
    "    f\"Prediction vs Observations. Predict start = {dt_start}, predict end = {dt_end}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict\n",
    "\n",
    "When latent space discovery is perfect then we can closely approximate reconstruction error.\n",
    "\n",
    "2 ways to predict:\n",
    "1) unroll the LSTM (ok for short term, not great for long term)\n",
    "2) integrate the discovered ODE forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict with discovered ODE model\n",
    "\n",
    "Predict from time = 0 or from the time starting the test period?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward simulation with the discovered SINDy model\n",
    "init_cond = np.zeros(driver._latent_dim)\n",
    "\n",
    "# t_test = np.arange(0, 365 * num_test_years, dt)\n",
    "# gru_outs_test_np = driver.gru_normalize(data_type=\"test\").detach().cpu().numpy()\n",
    "# init_cond[: driver._latent_dim] = gru_outs_test_np[0, :]\n",
    "\n",
    "t_test = np.arange(0, 365 * num_test_years, dt)\n",
    "gru_outs_test_np = driver.gru_normalize(data_type=\"test\").detach().cpu().numpy()\n",
    "gru_outs_train_np = driver.gru_normalize(data_type=\"train\").detach().cpu().numpy()\n",
    "init_cond[: driver._latent_dim] = gru_outs_train_np[-1, :]\n",
    "\n",
    "x_sim_test = driver._model.simulate(init_cond, t_test)\n",
    "x_sim_test = np.array(x_sim_test)  # Ensure it's a numpy array if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(\n",
    "    driver._latent_dim, 1, sharex=True, sharey=True, figsize=(10, 10)\n",
    ")\n",
    "for n in range(driver._latent_dim):\n",
    "    ax = axes[n]\n",
    "    ax.plot(x_sim_test[:, n], \"k--\", label=\"identified model\")\n",
    "    ax.plot(gru_outs_test_np[:, n], label=\"SINDy-SHRED latent space\")\n",
    "ax.legend()\n",
    "ax.set_ylim(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_sim_test[:, 2], \"k--\")\n",
    "plt.plot(gru_outs_test_np[:, 2])\n",
    "plt.ylim(-2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward simulation with the discovered SINDy model\n",
    "# t_test = np.arange(0, 365 * num_test_years, dt)\n",
    "# init_cond = np.zeros(driver._latent_dim)\n",
    "# # init_cond[:latent_dim] = gru_outs_test_np[0, :]\n",
    "# gru_outs_test_np = driver.gru_normalize(data_type=\"test\").detach().cpu().numpy()\n",
    "# init_cond[: driver._latent_dim] = gru_outs_test_np[0, :]\n",
    "\n",
    "# x_sim_test = driver._model.simulate(init_cond, t_test)\n",
    "\n",
    "# # Step 1: Reverse Min-Max scaling for SINDy-simulated data (x_sim_test)\n",
    "# x_sim_test = np.array(x_sim_test)  # Ensure it's a numpy array if needed\n",
    "\n",
    "# Revert the scaling from [-1, 1] back to [0, 1]\n",
    "x_sim_test = (x_sim_test + 1) / 2\n",
    "\n",
    "gru_out_train, _ = driver._shred.gru_outputs(driver._train_data.X, sindy=True)\n",
    "gru_out_train = gru_out_train[:, 0, :]\n",
    "gru_out_train = gru_out_train.detach().cpu().numpy()\n",
    "\n",
    "# Perform the Min-Max reverse transformation using the original min/max values\n",
    "for n in range(driver._latent_dim):  # Assuming 3 latent dimensions, need to be updated\n",
    "    x_sim_test[:, n] = x_sim_test[:, n] * (\n",
    "        np.max(gru_out_train[:, n]) - np.min(gru_out_train[:, n])\n",
    "    ) + np.min(gru_out_train[:, n])\n",
    "\n",
    "# Perform the decoder reconstruction using the transformed SINDy-simulated data\n",
    "latent_pred_sindy = torch.FloatTensor(x_sim_test).to(\n",
    "    device\n",
    ")  # Convert to torch tensor for reconstruction\n",
    "\n",
    "# Pass the SINDy-simulated latent space data through the decoder\n",
    "decoder_model = driver._shred\n",
    "output_sindy = decoder_model.linear1(latent_pred_sindy)\n",
    "output_sindy = decoder_model.dropout(output_sindy)\n",
    "output_sindy = torch.nn.functional.relu(output_sindy)\n",
    "output_sindy = decoder_model.linear2(output_sindy)\n",
    "output_sindy = decoder_model.dropout(output_sindy)\n",
    "output_sindy = torch.nn.functional.relu(output_sindy)\n",
    "output_sindy = decoder_model.linear3(output_sindy)\n",
    "\n",
    "# Detach and convert the reconstructed data back to numpy for visualization\n",
    "output_sindy_np = output_sindy.detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if modes_to_fit == \"ONI\":\n",
    "\n",
    "    time_results = ds_oni.time.values[t_ind]\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "\n",
    "    # Test\n",
    "    t = time_results[\n",
    "        lags\n",
    "        + train_length\n",
    "        + validate_length\n",
    "        - 2 : lags\n",
    "        + train_length\n",
    "        + validate_length\n",
    "        + test_length\n",
    "        - 2\n",
    "    ]\n",
    "    \n",
    "    # plt.plot(t, driver._scaler.inverse_transform(output_sindy_np).mean(axis=1))\n",
    "\n",
    "    \n",
    "    ds_results = xarray_unstacked(\n",
    "        driver._scaler.inverse_transform(output_sindy_np).T,\n",
    "        t,\n",
    "        lon_ragged_oni,\n",
    "        lat_ragged_oni,\n",
    "    )\n",
    "    enso_component = enso_box_extract(ds_results, enso34_box)\n",
    "    enso_spatial_mean = enso_component.mean(dim=[\"longitude\", \"latitude\"])\n",
    "    plt.plot(\n",
    "        t,\n",
    "        enso_spatial_mean,\n",
    "        color=\"xkcd:blue\",\n",
    "        ls=\":\",\n",
    "    )\n",
    "\n",
    "    # Observations\n",
    "    plt.plot(\n",
    "        background_anomaly.time, background_anomaly, color=\"k\", label=\"ENSO34 Index\"\n",
    "    )\n",
    "\n",
    "    plt.xlim(\n",
    "        time_results[lags],\n",
    "        time_results[train_length + validate_length + test_length + lags],\n",
    "    )\n",
    "    plt.xlim(time_results[lags], None)\n",
    "    plt.legend(ncols=3)\n",
    "    plt.ylabel(\"ENSO 34 box anomaly ($^{\\circ}C$)\")\n",
    "    plt.gca().axvline(time_results[train_length + lags - 2], ls=\"--\", color=\"0.5\")\n",
    "    plt.gca().axvline(\n",
    "        time_results[train_length + validate_length + lags - 2], ls=\"--\", color=\"0.5\"\n",
    "    )\n",
    "    plt.title(f\"fit {modes_to_fit} data\\n Comparison in ENSO34 box\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Unroll LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a SHRED model to generate the LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sensor_train_data_out = torch.tensor(\n",
    "#     transformed_X[train_indices + lags][:, sensor_locations], dtype=torch.float32\n",
    "# ).to(device)\n",
    "# sensor_valid_data_out = torch.tensor(\n",
    "#     transformed_X[valid_indices + lags][:, sensor_locations], dtype=torch.float32\n",
    "# ).to(device)\n",
    "# sensor_test_data_out = torch.tensor(\n",
    "#     transformed_X[test_indices + lags][:, sensor_locations], dtype=torch.float32\n",
    "# ).to(device)\n",
    "\n",
    "# sensor_train_dataset = TimeSeriesDataset(train_data_in, sensor_train_data_out)\n",
    "# sensor_valid_dataset = TimeSeriesDataset(valid_data_in, sensor_valid_data_out)\n",
    "# sensor_test_dataset = TimeSeriesDataset(test_data_in, sensor_test_data_out)\n",
    "\n",
    "sensor_forecaster = models.SHRED(\n",
    "    num_sensors,\n",
    "    num_sensors,\n",
    "    hidden_size=32,\n",
    "    hidden_layers=2,\n",
    "    l1=100,\n",
    "    l2=150,\n",
    "    dropout=0.1,\n",
    ").to(device)\n",
    "\n",
    "sensor_val_errors = models.fit(\n",
    "    sensor_forecaster,\n",
    "    driver._train_data,\n",
    "    driver._valid_data,\n",
    "    batch_size=64,\n",
    "    num_epochs=1000,\n",
    "    verbose=True,\n",
    "    lr=1e-3,\n",
    "    patience=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = driver._test_data\n",
    "initialization = test_dataset.X[0:1].clone()\n",
    "\n",
    "forecasted_sensors, forecasted_reconstructions = models.forecast(\n",
    "    sensor_forecaster, driver._shred, test_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasted_long_sensors = np.zeros((forecasted_sensors.shape[0], len_space))\n",
    "for i in range(len(forecasted_long_sensors)):\n",
    "    forecasted_long_sensors[i, sensor_locations] = (\n",
    "        forecasted_sensors[i].detach().cpu().numpy()\n",
    "    )\n",
    "\n",
    "scaled_forecast = driver._scaler.inverse_transform(\n",
    "    forecasted_reconstructions.reshape(-1, len_space)\n",
    ")\n",
    "truths = np.zeros_like(scaled_forecast)\n",
    "for i in range(len(forecasted_reconstructions)):\n",
    "    truth = driver._scaler.inverse_transform(\n",
    "        driver._train_data.Y[i : i + 1].detach().cpu().numpy()\n",
    "    )\n",
    "    truths[i] = truth.reshape(scaled_forecast.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in [0, 53, 67, 451, 312]:\n",
    "    plt.plot(truths[:, n], \"r\")\n",
    "    plt.plot(forecasted_long_sensors[:, n], color=\"k\", ls=\"--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multiple_sensor_predictions(\n",
    "    real_data,\n",
    "    sindy_data,\n",
    "    sensor_locations,\n",
    "    sensor_indices,\n",
    "    num_train=52,\n",
    "    num_pred=250,\n",
    "    rows=4,\n",
    "    cols=4,\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot the real data and SINDy-SHRED prediction for multiple sensors in a grid layout,\n",
    "    normalizing each plot to (0,1) based on the min/max of the real data for that sensor.\n",
    "\n",
    "    Args:\n",
    "        real_data (np.array): The real SST data.\n",
    "        sindy_data (np.array): The predicted data from SINDy-SHRED.\n",
    "        sensor_locations (np.array): Sensor locations (indices) used in training.\n",
    "        sensor_indices (list): Indices of the sensors to visualize.\n",
    "        num_train (int): Number of training data points (default is 52).\n",
    "        num_pred (int): Number of prediction data points (default is 250).\n",
    "        rows (int): Number of rows in the grid layout.\n",
    "        cols (int): Number of columns in the grid layout.\n",
    "\n",
    "    Returns:\n",
    "        None. Displays and saves the plot.\n",
    "    \"\"\"\n",
    "    num_sensors = len(sensor_indices)\n",
    "    fig, axes = plt.subplots(\n",
    "        rows, cols, figsize=(3 * cols, 2 * rows), sharex=True, sharey=True\n",
    "    )\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, sensor_idx in enumerate(sensor_indices):\n",
    "        if i >= rows * cols:\n",
    "            break\n",
    "        sensor = sensor_locations[sensor_idx]\n",
    "        # Real data for the training + prediction period\n",
    "        sensor_real_data = real_data[: num_train + num_pred, sensor]\n",
    "        # SINDy-SHRED prediction for the prediction period\n",
    "        sensor_pred_data = sindy_data[num_train : num_train + num_pred, sensor]\n",
    "\n",
    "        # Normalize both real and predicted data to (0,1) based on real data min/max\n",
    "        min_val = np.min(sensor_real_data)\n",
    "        max_val = np.max(sensor_real_data)\n",
    "        if max_val - min_val == 0:\n",
    "            # Avoid division by zero; plot as zeros\n",
    "            norm_real = np.zeros_like(sensor_real_data)\n",
    "            norm_pred = np.zeros_like(sensor_pred_data)\n",
    "        else:\n",
    "            norm_real = (sensor_real_data - min_val) / (max_val - min_val)\n",
    "            norm_pred = (sensor_pred_data - min_val) / (max_val - min_val)\n",
    "\n",
    "        axes[i].plot(\n",
    "            np.arange(num_train + num_pred),\n",
    "            norm_real,\n",
    "            color=\"blue\",\n",
    "            linewidth=2,\n",
    "            label=\"Real Data\",\n",
    "        )\n",
    "        axes[i].plot(\n",
    "            np.arange(num_train, num_train + num_pred),\n",
    "            norm_pred,\n",
    "            color=\"red\",\n",
    "            linestyle=\"--\",\n",
    "            linewidth=2,\n",
    "            label=\"SINDy-SHRED\",\n",
    "        )\n",
    "        axes[i].set_title(f\"Sensor {sensor_idx}\", fontsize=12)\n",
    "        axes[i].tick_params(axis=\"both\", which=\"major\", labelsize=10)\n",
    "        axes[i].axvline(x=num_train, color=\"gray\", linestyle=\":\", linewidth=1.5)\n",
    "        axes[i].set_ylim(0, 1)\n",
    "\n",
    "    # Remove any unused subplots\n",
    "    for i in range(num_sensors, rows * cols):\n",
    "        fig.delaxes(axes[i])\n",
    "\n",
    "    fig.text(0.5, 0.0, \"Time (weeks)\", ha=\"center\", va=\"center\", fontsize=16)\n",
    "    # Add a common legend\n",
    "    lines = [\n",
    "        plt.Line2D([0], [0], color=\"blue\", lw=2),\n",
    "        plt.Line2D([0], [0], color=\"red\", linestyle=\"--\", lw=2),\n",
    "    ]\n",
    "    labels = [\"Real Data\", \"SINDy-SHRED\"]\n",
    "    fig.legend(\n",
    "        lines,\n",
    "        labels,\n",
    "        loc=\"upper right\",\n",
    "        bbox_to_anchor=(0.99, 0.945),\n",
    "        fontsize=10,\n",
    "        frameon=False,\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\n",
    "        \"sensor_predictions_grid.pdf\", format=\"pdf\", bbox_inches=\"tight\", dpi=300\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# Choose a set of sensor indices to visualize (e.g., 16 sensors)\n",
    "num_sensors_to_plot = 16\n",
    "sensor_indices = list(range(num_sensors_to_plot))\n",
    "# Use the same sst_locs as above for sensor_locations\n",
    "sensor_locations = np.random.randint(1, 40000, size=18)\n",
    "plot_multiple_sensor_predictions(\n",
    "    real_data=truths,\n",
    "    sindy_data=scaled_forecast,\n",
    "    sensor_locations=sensor_locations,\n",
    "    sensor_indices=sensor_indices,\n",
    "    num_train=52,\n",
    "    num_pred=250,\n",
    "    rows=4,\n",
    "    cols=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code from `sindy_shred.forecast`\n",
    "\n",
    "Outstanding questions:\n",
    "\n",
    "- Still unclear what `forecaster` is supposed to be.\n",
    "- Doesn't match the SHRED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = driver._test_data\n",
    "\n",
    "initial_in = test_dataset.X[0:1].clone()\n",
    "vals = [\n",
    "    initial_in[0, i, :].detach().cpu().clone().numpy()\n",
    "    for i in range(test_dataset.X.shape[1])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecaster = sensor_forecaster\n",
    "# t_forecast = np.arange(0, len(test_dataset.X[0]) * driver._dt, driver._dt)\n",
    "\n",
    "for i in range(len(test_dataset.X)):\n",
    "    scaled_output1, scaled_output2 = forecaster(initial_in)\n",
    "    scaled_output1 = scaled_output1.detach().cpu().numpy()\n",
    "    scaled_output2 = scaled_output2.detach().cpu().numpy()\n",
    "    vals.append(\n",
    "        np.concatenate(\n",
    "            [\n",
    "                scaled_output1.reshape(test_dataset.X.shape[2] // 2),\n",
    "                scaled_output2.reshape(test_dataset.X.shape[2] // 2),\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "    temp = initial_in.clone()\n",
    "    initial_in[0, :-1] = temp[0, 1:]\n",
    "    initial_in[0, -1] = torch.tensor(np.concatenate([scaled_output1, scaled_output2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sindyshred",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
