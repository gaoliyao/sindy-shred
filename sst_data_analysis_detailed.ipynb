{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-header",
   "metadata": {},
   "source": [
    "# SINDy-SHRED Applied to SST Dataset (Low-Level API)\n",
    "\n",
    "This notebook demonstrates the detailed workflow of SINDy-SHRED using the low-level API.\n",
    "For a simpler high-level interface, see `sst_sindy_shred_refactor.ipynb`.\n",
    "\n",
    "**SHRED** (SHallow REcurrent Decoder) models combine a recurrent layer (GRU) with a shallow decoder network to reconstruct high-dimensional spatio-temporal fields from sensor measurements.\n",
    "\n",
    "**SINDy-SHRED** extends this by integrating Sparse Identification of Nonlinear Dynamics (SINDy):\n",
    "\n",
    "$$\\dot{z} = \\Theta(z) \\xi$$\n",
    "\n",
    "where $z$ is the latent space, $\\Theta(z)$ is a library of candidate functions, and $\\xi$ is a sparse coefficient matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pysindy as ps\n",
    "import torch\n",
    "from scipy.io import loadmat\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import plotting\n",
    "\n",
    "# Local modules\n",
    "import sindy\n",
    "import sindy_shred_net\n",
    "from processdata import load_data\n",
    "from utils import TimeSeriesDataset, get_device\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Create results directory\n",
    "RESULTS_DIR = \"results/sst\"\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "print(f\"Results will be saved to: {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "device-header",
   "metadata": {},
   "source": [
    "### Device and Seed Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "device-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device selection\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "SEED = 0\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-header",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-load",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SST data\n",
    "load_X = load_data(\"SST\")\n",
    "n = load_X.shape[0]  # Number of time samples\n",
    "m = load_X.shape[1]  # Spatial dimension\n",
    "\n",
    "print(f\"Data shape: {load_X.shape}\")\n",
    "print(f\"Number of time samples: {n}\")\n",
    "print(f\"Spatial dimension: {m}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sensor configuration\n",
    "num_sensors = 250\n",
    "sensor_locations = np.random.choice(m, size=num_sensors, replace=False)\n",
    "\n",
    "# Trajectory length (52 weeks = 1 year of measurements)\n",
    "lags = 52\n",
    "\n",
    "# Data split lengths\n",
    "train_length = 1000\n",
    "validate_length = 30\n",
    "\n",
    "# Time step (weekly data)\n",
    "dt = 1 / 52.0\n",
    "\n",
    "# Model hyperparameters\n",
    "latent_dim = 3\n",
    "poly_order = 1\n",
    "include_sine = False\n",
    "\n",
    "# Calculate library dimension\n",
    "library_dim = sindy.library_size(\n",
    "    latent_dim, poly_order, include_sine, include_constant=True\n",
    ")\n",
    "\n",
    "print(f\"Number of sensors: {num_sensors}\")\n",
    "print(f\"Trajectory length (lags): {lags}\")\n",
    "print(f\"Training length: {train_length}\")\n",
    "print(f\"Latent dimension: {latent_dim}\")\n",
    "print(f\"Library dimension: {library_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preprocess-header",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "\n",
    "Split data into train/validation/test sets and apply MinMax scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/validation/test indices\n",
    "train_indices = np.arange(0, train_length)\n",
    "\n",
    "mask = np.ones(n - lags)\n",
    "mask[train_indices] = 0\n",
    "valid_test_indices = np.arange(0, n - lags)[np.where(mask != 0)[0]]\n",
    "valid_indices = valid_test_indices[:validate_length]\n",
    "test_indices = valid_test_indices[validate_length:]\n",
    "\n",
    "print(f\"Train samples: {len(train_indices)}\")\n",
    "print(f\"Validation samples: {len(valid_indices)}\")\n",
    "print(f\"Test samples: {len(test_indices)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scale-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data using MinMaxScaler (fit on training data only)\n",
    "sc = MinMaxScaler()\n",
    "sc = sc.fit(load_X[train_indices])\n",
    "transformed_X = sc.transform(load_X)\n",
    "\n",
    "# Generate input sequences (sensor trajectories)\n",
    "all_data_in = np.zeros((n - lags, lags, num_sensors))\n",
    "for i in range(len(all_data_in)):\n",
    "    all_data_in[i] = transformed_X[i : i + lags, sensor_locations]\n",
    "\n",
    "# Create input tensors\n",
    "train_data_in = torch.tensor(all_data_in[train_indices], dtype=torch.float32).to(device)\n",
    "valid_data_in = torch.tensor(all_data_in[valid_indices], dtype=torch.float32).to(device)\n",
    "test_data_in = torch.tensor(all_data_in[test_indices], dtype=torch.float32).to(device)\n",
    "\n",
    "# Create output tensors (-1 to align with final sensor measurement)\n",
    "train_data_out = torch.tensor(\n",
    "    transformed_X[train_indices + lags - 1], dtype=torch.float32\n",
    ").to(device)\n",
    "valid_data_out = torch.tensor(\n",
    "    transformed_X[valid_indices + lags - 1], dtype=torch.float32\n",
    ").to(device)\n",
    "test_data_out = torch.tensor(\n",
    "    transformed_X[test_indices + lags - 1], dtype=torch.float32\n",
    ").to(device)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TimeSeriesDataset(train_data_in, train_data_out)\n",
    "valid_dataset = TimeSeriesDataset(valid_data_in, valid_data_out)\n",
    "test_dataset = TimeSeriesDataset(test_data_in, test_data_out)\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Input shape: {train_data_in.shape}\")\n",
    "print(f\"Output shape: {train_data_out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz-data-header",
   "metadata": {},
   "source": [
    "### Visualize Sensor Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sample sensor time series\n",
    "n_sensors_to_plot = min(num_sensors, 10)\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    n_sensors_to_plot, 1, figsize=(10, 2 * n_sensors_to_plot), sharex=True\n",
    ")\n",
    "for i in range(n_sensors_to_plot):\n",
    "    axes[i].plot(load_X[:, sensor_locations[i]])\n",
    "    axes[i].set_ylabel(f\"Sensor {i}\")\n",
    "axes[-1].set_xlabel(\"Time (weeks)\")\n",
    "fig.suptitle(\"Sample Sensor Time Series\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-header",
   "metadata": {},
   "source": [
    "## 4. Model Creation and Training\n",
    "\n",
    "Create the SINDy-SHRED network and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SINDy-SHRED network\n",
    "shred = sindy_shred_net.SINDy_SHRED_net(\n",
    "    input_size=num_sensors,\n",
    "    output_size=m,\n",
    "    hidden_size=latent_dim,\n",
    "    hidden_layers=2,\n",
    "    l1=350,\n",
    "    l2=400,\n",
    "    dropout=0.1,\n",
    "    library_dim=library_dim,\n",
    "    poly_order=poly_order,\n",
    "    include_sine=include_sine,\n",
    "    dt=dt,\n",
    ").to(device)\n",
    "\n",
    "print(\"SINDy-SHRED network created\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in shred.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "validation_errors = sindy_shred_net.fit(\n",
    "    shred,\n",
    "    train_dataset,\n",
    "    valid_dataset,\n",
    "    batch_size=128,\n",
    "    num_epochs=600,\n",
    "    lr=1e-3,\n",
    "    verbose=True,\n",
    "    threshold=0.05,\n",
    "    patience=5,\n",
    "    sindy_regularization=10.0,\n",
    "    optimizer=\"AdamW\",\n",
    "    thres_epoch=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sparsity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check sparsity of learned coefficients\n",
    "sparsity_rate = torch.mean(shred.e_sindy.coefficient_mask * 1.0)\n",
    "print(f\"Coefficient sparsity rate: {sparsity_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval-header",
   "metadata": {},
   "source": [
    "## 5. Evaluation\n",
    "\n",
    "Evaluate reconstruction performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval-recon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute test reconstruction error\n",
    "test_recons = sc.inverse_transform(shred(test_dataset.X).detach().cpu().numpy())\n",
    "test_ground_truth = sc.inverse_transform(test_dataset.Y.detach().cpu().numpy())\n",
    "\n",
    "relative_error = np.linalg.norm(test_recons - test_ground_truth) / np.linalg.norm(\n",
    "    test_ground_truth\n",
    ")\n",
    "print(f\"Test set relative reconstruction error: {relative_error:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sindy-header",
   "metadata": {},
   "source": [
    "## 6. Post-hoc SINDy Discovery\n",
    "\n",
    "Extract latent trajectories and discover sparse governing equations using PySINDy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extract-latent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract latent trajectories from training data\n",
    "gru_outs_train, _ = shred.gru_outputs(train_dataset.X, sindy=True)\n",
    "gru_outs_train = gru_outs_train[:, 0, :]\n",
    "\n",
    "# Normalize latent trajectories to [-1, 1]\n",
    "gru_outs_normalized = gru_outs_train.clone()\n",
    "for i in range(latent_dim):\n",
    "    gru_outs_normalized[:, i] = (\n",
    "        gru_outs_train[:, i] - torch.min(gru_outs_train[:, i])\n",
    "    ) / (torch.max(gru_outs_train[:, i]) - torch.min(gru_outs_train[:, i]))\n",
    "gru_outs_normalized = 2 * gru_outs_normalized - 1\n",
    "\n",
    "x_train = gru_outs_normalized.detach().cpu().numpy()\n",
    "print(f\"Latent trajectories shape: {x_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sindy-discover",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SINDy discovery\n",
    "sindy_threshold = 0.05\n",
    "\n",
    "differentiation_method = ps.differentiation.FiniteDifference()\n",
    "# For noisy data, consider: ps.differentiation.SmoothedFiniteDifference()\n",
    "\n",
    "model = ps.SINDy(\n",
    "    optimizer=ps.STLSQ(threshold=sindy_threshold, alpha=0.05),\n",
    "    differentiation_method=differentiation_method,\n",
    "    feature_library=ps.PolynomialLibrary(degree=poly_order),\n",
    ")\n",
    "\n",
    "model.fit(x_train, t=dt)\n",
    "print(\"\\nDiscovered SINDy equations:\")\n",
    "model.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sindy-simulate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate discovered model\n",
    "t_sim = np.arange(0, len(x_train) * dt, dt)\n",
    "init_cond = x_train[0, :]\n",
    "x_sim = model.simulate(init_cond, t_sim)\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(latent_dim, 1, figsize=(10, 2 * latent_dim), sharex=True)\n",
    "for i in range(latent_dim):\n",
    "    axes[i].plot(t_sim, x_train[: len(t_sim), i], label=\"SINDy-SHRED\")\n",
    "    axes[i].plot(t_sim, x_sim[:, i], \"k--\", label=\"Identified model\")\n",
    "    axes[i].set_ylabel(rf\"$z_{{{i}}}$\")\n",
    "    if i == latent_dim - 1:\n",
    "        axes[i].set_xlabel(\"Time\")\n",
    "        axes[i].legend()\n",
    "\n",
    "fig.suptitle(\"Latent Space: SINDy-SHRED vs Identified Model\")\n",
    "fig.tight_layout()\n",
    "fig.savefig(f\"{RESULTS_DIR}/latent_comparison.pdf\", bbox_inches=\"tight\", dpi=300)\n",
    "fig.savefig(f\"{RESULTS_DIR}/latent_comparison.png\", bbox_inches=\"tight\", dpi=300)\n",
    "print(f\"Saved latent comparison plot to {RESULTS_DIR}/latent_comparison.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "predict-header",
   "metadata": {},
   "source": [
    "## 7. SINDy Prediction and Decoding\n",
    "\n",
    "Use the discovered SINDy model to predict latent trajectories and decode back to physical space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "predict-latent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test latent trajectories and normalize\n",
    "gru_outs_test, _ = shred.gru_outputs(test_dataset.X, sindy=True)\n",
    "gru_outs_test = gru_outs_test[:, 0, :]\n",
    "gru_outs_train_np = gru_outs_train.detach().cpu().numpy()\n",
    "\n",
    "# Normalize test using training statistics\n",
    "gru_outs_test_np = gru_outs_test.detach().cpu().numpy()\n",
    "for i in range(latent_dim):\n",
    "    gru_outs_test_np[:, i] = (\n",
    "        gru_outs_test_np[:, i] - np.min(gru_outs_train_np[:, i])\n",
    "    ) / (np.max(gru_outs_train_np[:, i]) - np.min(gru_outs_train_np[:, i]))\n",
    "gru_outs_test_np = 2 * gru_outs_test_np - 1\n",
    "\n",
    "# Simulate SINDy model from test initial condition\n",
    "t_test = np.arange(0, len(test_indices) * dt, dt)\n",
    "init_cond_test = gru_outs_test_np[0, :]\n",
    "x_predict = model.simulate(init_cond_test, t_test)\n",
    "\n",
    "print(f\"SINDy prediction shape: {x_predict.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decode-sindy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode SINDy predictions to physical space\n",
    "# Step 1: Reverse normalization\n",
    "x_denorm = (x_predict + 1) / 2  # [-1,1] -> [0,1]\n",
    "for i in range(latent_dim):\n",
    "    x_denorm[:, i] = x_denorm[:, i] * (\n",
    "        np.max(gru_outs_train_np[:, i]) - np.min(gru_outs_train_np[:, i])\n",
    "    ) + np.min(gru_outs_train_np[:, i])\n",
    "\n",
    "# Step 2: Pass through decoder\n",
    "latent_pred = torch.FloatTensor(x_denorm).to(device)\n",
    "output = shred.linear1(latent_pred)\n",
    "output = shred.dropout(output)\n",
    "output = torch.nn.functional.relu(output)\n",
    "output = shred.linear2(output)\n",
    "output = shred.dropout(output)\n",
    "output = torch.nn.functional.relu(output)\n",
    "output = shred.linear3(output)\n",
    "\n",
    "output_sindy = output.detach().cpu().numpy()\n",
    "print(f\"Decoded output shape: {output_sindy.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz-header",
   "metadata": {},
   "source": [
    "## 8. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-sst-locs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SST location indices for visualization\n",
    "load_X_full = loadmat(\"Data/SST_data.mat\")[\"Z\"].T\n",
    "mean_X = np.mean(load_X_full, axis=0)\n",
    "sst_locs = np.where(mean_X != 0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spatial-compare",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spatial reconstruction comparison\n",
    "timesteps = [0, 50, 75, 100, 125]\n",
    "test_Y = test_dataset.Y.detach().cpu().numpy()\n",
    "\n",
    "fig, axes = plotting.plot_reconstruction_comparison(\n",
    "    test_Y,\n",
    "    output_sindy,\n",
    "    timesteps,\n",
    "    sst_locs=sst_locs,\n",
    "    lat_range=(0, 180),\n",
    "    lon_range=(0, 180),\n",
    "    diff_scale=10,\n",
    ")\n",
    "fig.suptitle(\"Spatial Reconstruction: Real vs SINDy-Predicted\")\n",
    "fig.savefig(f\"{RESULTS_DIR}/spatial_comparison.pdf\", bbox_inches=\"tight\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sensor-pred",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sensor-level predictions\n",
    "sensor_locations_viz = np.random.randint(1, 40000, size=18)\n",
    "\n",
    "fig, axes = plotting.plot_sensor_predictions(\n",
    "    test_Y,\n",
    "    output_sindy,\n",
    "    sensor_locations=sensor_locations_viz,\n",
    "    num_context=lags,\n",
    "    num_pred=min(250, len(output_sindy) - lags),\n",
    "    rows=3,\n",
    "    cols=6,\n",
    "    save_path=f\"{RESULTS_DIR}/sensor_predictions_grid.pdf\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-header",
   "metadata": {},
   "source": [
    "## 9. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "from io import StringIO\n",
    "\n",
    "# Save latent trajectories\n",
    "np.save(f\"{RESULTS_DIR}/latent_train.npy\", x_train)\n",
    "np.save(f\"{RESULTS_DIR}/latent_test.npy\", gru_outs_test_np)\n",
    "np.save(f\"{RESULTS_DIR}/latent_sindy_predict.npy\", x_predict)\n",
    "print(f\"Saved latent trajectories\")\n",
    "\n",
    "# Save SINDy coefficients\n",
    "sindy_coefficients = model.coefficients()\n",
    "np.save(f\"{RESULTS_DIR}/sindy_coefficients.npy\", sindy_coefficients)\n",
    "print(f\"Saved SINDy coefficients: shape {sindy_coefficients.shape}\")\n",
    "\n",
    "# Save feature names\n",
    "feature_names = model.get_feature_names()\n",
    "with open(f\"{RESULTS_DIR}/sindy_feature_names.txt\", \"w\") as f:\n",
    "    for name in feature_names:\n",
    "        f.write(name + \"\\n\")\n",
    "\n",
    "# Save SINDy equations\n",
    "old_stdout = sys.stdout\n",
    "sys.stdout = StringIO()\n",
    "model.print()\n",
    "equations_str = sys.stdout.getvalue()\n",
    "sys.stdout = old_stdout\n",
    "\n",
    "with open(f\"{RESULTS_DIR}/sindy_equations.txt\", \"w\") as f:\n",
    "    f.write(\"Discovered SINDy Equations:\\n\")\n",
    "    f.write(\"=\" * 40 + \"\\n\")\n",
    "    f.write(equations_str)\n",
    "print(f\"Saved SINDy equations to {RESULTS_DIR}/sindy_equations.txt\")\n",
    "\n",
    "# Save config as JSON\n",
    "config = {\n",
    "    \"latent_dim\": latent_dim,\n",
    "    \"poly_order\": poly_order,\n",
    "    \"num_sensors\": num_sensors,\n",
    "    \"lags\": lags,\n",
    "    \"train_length\": train_length,\n",
    "    \"validate_length\": validate_length,\n",
    "    \"dt\": dt,\n",
    "    \"sindy_threshold\": sindy_threshold,\n",
    "}\n",
    "with open(f\"{RESULTS_DIR}/config.json\", \"w\") as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "print(f\"Saved configuration to {RESULTS_DIR}/config.json\")\n",
    "\n",
    "# Compute additional errors for results\n",
    "# SINDy latent prediction error (training data)\n",
    "sindy_latent_error = np.linalg.norm(x_sim - x_train[: len(x_sim)]) / np.linalg.norm(\n",
    "    x_train[: len(x_sim)]\n",
    ")\n",
    "\n",
    "# SINDy physical prediction error (test data, use min length to handle shape mismatch)\n",
    "output_sindy_unscaled = sc.inverse_transform(output_sindy)\n",
    "n_compare = min(len(output_sindy_unscaled), len(test_ground_truth))\n",
    "sindy_physical_error = np.linalg.norm(\n",
    "    output_sindy_unscaled[:n_compare] - test_ground_truth[:n_compare]\n",
    ") / np.linalg.norm(test_ground_truth[:n_compare])\n",
    "\n",
    "# Save results as JSON\n",
    "results = {\n",
    "    \"reconstruction_error\": float(relative_error),\n",
    "    \"sindy_latent_error\": float(sindy_latent_error),\n",
    "    \"sindy_prediction_error\": float(sindy_physical_error),\n",
    "}\n",
    "with open(f\"{RESULTS_DIR}/results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print(f\"Saved results to {RESULTS_DIR}/results.json\")\n",
    "\n",
    "print(\"\\nAll results saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sindyshred",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
